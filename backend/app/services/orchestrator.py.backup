"""Orchestrator service for managing agent workflows with dynamic workflow execution."""

from typing import List, Optional, Dict, Any
from uuid import UUID, uuid4
from datetime import datetime, timedelta, timezone
from sqlalchemy.orm import Session
import structlog
import autogen_agentchat as autogen

from app.models.task import Task, TaskStatus
from app.models.agent import AgentType, AgentStatus
from app.models.handoff import HandoffSchema
from app.database.models import TaskDB, AgentStatusDB, ProjectDB, HitlRequestDB
from app.models.hitl import HitlStatus
# Import moved inside submit_task() to avoid circular import
from app.websocket.manager import websocket_manager
from app.websocket.events import WebSocketEvent, EventType
from app.services.context_store import ContextStoreService
from app.services.autogen_service import AutoGenService
from app.services.workflow_engine import WorkflowExecutionEngine
from app.services.conflict_resolver import ConflictResolverService

logger = structlog.get_logger(__name__)

# Enhanced 6-Phase SDLC Orchestration Configuration with Time-Conscious Features
SDLC_PHASES = {
    "discovery": {
        "name": "Discovery",
        "description": "Requirements gathering and initial analysis",
        "agent_sequence": ["analyst"],
        "completion_criteria": ["user_input_analyzed", "requirements_gathered"],
        "estimated_duration_hours": 4,
        "max_duration_hours": 6,  # Maximum allowed time before escalation
        "time_pressure_threshold": 0.8,  # 80% of estimated time triggers warnings
        "parallel_execution": False,
        "time_based_decisions": {
            "overtime_action": "escalate_to_hitl",
            "efficiency_bonus": "reduce_analysis_depth"
        },
        "next_phase": "plan"
    },
    "plan": {
        "name": "Plan",
        "description": "Technical planning and architecture design",
        "agent_sequence": ["architect"],
        "completion_criteria": ["architecture_defined", "technical_plan_created"],
        "estimated_duration_hours": 8,
        "max_duration_hours": 12,
        "time_pressure_threshold": 0.75,
        "parallel_execution": False,
        "time_based_decisions": {
            "overtime_action": "simplify_architecture",
            "efficiency_bonus": "add_performance_optimization"
        },
        "next_phase": "design"
    },
    "design": {
        "name": "Design",
        "description": "Detailed design and API specification",
        "agent_sequence": ["architect", "analyst"],
        "completion_criteria": ["api_specs_defined", "data_models_created", "design_reviewed"],
        "estimated_duration_hours": 12,
        "max_duration_hours": 18,
        "time_pressure_threshold": 0.7,
        "parallel_execution": True,  # Allow parallel execution of architect and analyst
        "time_based_decisions": {
            "overtime_action": "prioritize_critical_components",
            "efficiency_bonus": "enhance_design_quality"
        },
        "next_phase": "build"
    },
    "build": {
        "name": "Build",
        "description": "Code implementation and development",
        "agent_sequence": ["coder"],
        "completion_criteria": ["code_implemented", "unit_tests_passed", "code_reviewed"],
        "estimated_duration_hours": 24,
        "max_duration_hours": 36,
        "time_pressure_threshold": 0.6,
        "parallel_execution": False,
        "time_based_decisions": {
            "overtime_action": "focus_on_core_features",
            "efficiency_bonus": "add_advanced_features"
        },
        "next_phase": "validate"
    },
    "validate": {
        "name": "Validate",
        "description": "Testing and quality assurance",
        "agent_sequence": ["tester"],
        "completion_criteria": ["tests_executed", "quality_gates_passed", "performance_validated"],
        "estimated_duration_hours": 8,
        "max_duration_hours": 12,
        "time_pressure_threshold": 0.8,
        "parallel_execution": False,
        "time_based_decisions": {
            "overtime_action": "prioritize_critical_tests",
            "efficiency_bonus": "comprehensive_testing"
        },
        "next_phase": "launch"
    },
    "launch": {
        "name": "Launch",
        "description": "Deployment and production release",
        "agent_sequence": ["deployer"],
        "completion_criteria": ["deployment_successful", "monitoring_configured", "documentation_complete"],
        "estimated_duration_hours": 4,
        "max_duration_hours": 6,
        "time_pressure_threshold": 0.9,
        "parallel_execution": False,
        "time_based_decisions": {
            "overtime_action": "simplified_deployment",
            "efficiency_bonus": "enhanced_monitoring"
        },
        "next_phase": None
    }
}


class OrchestratorService:
    """Service for orchestrating agent workflows with dynamic workflow execution."""

    def __init__(self, db: Session):
        self.db = db
        self.context_store = ContextStoreService(db)
        self.autogen_service = AutoGenService()
        self.workflow_engine = WorkflowExecutionEngine(db)
        self.conflict_resolver = ConflictResolverService(self.context_store, self.autogen_service)
        self.current_project_phases = {}  # Track current phase per project

    def get_current_phase(self, project_id: UUID) -> str:
        """Get the current SDLC phase for a project."""
        return self.current_project_phases.get(str(project_id), "discovery")

    def set_current_phase(self, project_id: UUID, phase: str):
        """Set the current SDLC phase for a project."""
        if phase not in SDLC_PHASES:
            raise ValueError(f"Invalid phase: {phase}")
        self.current_project_phases[str(project_id)] = phase
        logger.info("Project phase updated", project_id=project_id, phase=phase)

    def validate_phase_completion(self, project_id: UUID, phase: str) -> Dict[str, Any]:
        """
        Validate if a phase has met its completion criteria.

        Args:
            project_id: UUID of the project
            phase: Phase to validate

        Returns:
            Dict with validation results
        """
        if phase not in SDLC_PHASES:
            return {"valid": False, "error": f"Invalid phase: {phase}"}

        phase_config = SDLC_PHASES[phase]
        completion_criteria = phase_config["completion_criteria"]

        # Get project tasks for this phase
        project_tasks = self.get_project_tasks(project_id)
        phase_tasks = [task for task in project_tasks if task.agent_type in phase_config["agent_sequence"]]

        # Check completion criteria
        completed_criteria = []
        missing_criteria = []

        for criterion in completion_criteria:
            if self._check_completion_criterion(project_id, phase, criterion, phase_tasks):
                completed_criteria.append(criterion)
            else:
                missing_criteria.append(criterion)

        is_complete = len(missing_criteria) == 0

        result = {
            "valid": True,
            "phase": phase,
            "is_complete": is_complete,
            "completed_criteria": completed_criteria,
            "missing_criteria": missing_criteria,
            "completion_percentage": len(completed_criteria) / len(completion_criteria) * 100
        }

        logger.info("Phase completion validation",
                   project_id=project_id,
                   phase=phase,
                   is_complete=is_complete,
                   completion_percentage=result["completion_percentage"])

        return result

    def _check_completion_criterion(self, project_id: UUID, phase: str, criterion: str, phase_tasks: List[Task]) -> bool:
        """Check if a specific completion criterion is met."""
        # Get context artifacts for validation
        artifacts = self.context_store.get_artifacts_by_project(project_id)

        if criterion == "user_input_analyzed":
            # Check if analyst has processed user input
            analyst_tasks = [t for t in phase_tasks if t.agent_type == "analyst" and t.status == TaskStatus.COMPLETED]
            return len(analyst_tasks) > 0

        elif criterion == "requirements_gathered":
            # Check for requirements artifacts
            req_artifacts = [a for a in artifacts if "requirement" in a.artifact_type.lower()]
            return len(req_artifacts) > 0

        elif criterion == "architecture_defined":
            # Check for architecture artifacts
            arch_artifacts = [a for a in artifacts if "architecture" in a.artifact_type.lower()]
            return len(arch_artifacts) > 0

        elif criterion == "technical_plan_created":
            # Check for technical planning artifacts
            plan_artifacts = [a for a in artifacts if "plan" in a.artifact_type.lower()]
            return len(plan_artifacts) > 0

        elif criterion == "api_specs_defined":
            # Check for API specification artifacts
            api_artifacts = [a for a in artifacts if "api" in a.artifact_type.lower() or "spec" in a.artifact_type.lower()]
            return len(api_artifacts) > 0

        elif criterion == "data_models_created":
            # Check for data model artifacts
            model_artifacts = [a for a in artifacts if "model" in a.artifact_type.lower()]
            return len(model_artifacts) > 0

        elif criterion == "design_reviewed":
            # Check if design has been reviewed (HITL approved)
            design_artifacts = [a for a in artifacts if "design" in a.artifact_type.lower()]
            return len(design_artifacts) > 0

        elif criterion == "code_implemented":
            # Check for source code artifacts
            code_artifacts = [a for a in artifacts if a.artifact_type == "source_code"]
            return len(code_artifacts) > 0

        elif criterion == "unit_tests_passed":
            # Check for test result artifacts
            test_artifacts = [a for a in artifacts if "test" in a.artifact_type.lower()]
            return len(test_artifacts) > 0

        elif criterion == "code_reviewed":
            # Check if code has been reviewed (HITL approved)
            code_artifacts = [a for a in artifacts if a.artifact_type == "source_code"]
            return len(code_artifacts) > 0

        elif criterion == "tests_executed":
            # Check for test execution artifacts
            test_artifacts = [a for a in artifacts if "test" in a.artifact_type.lower()]
            return len(test_artifacts) > 0

        elif criterion == "quality_gates_passed":
            # Check for quality gate artifacts
            quality_artifacts = [a for a in artifacts if "quality" in a.artifact_type.lower() or "gate" in a.artifact_type.lower()]
            return len(quality_artifacts) > 0

        elif criterion == "performance_validated":
            # Check for performance test artifacts
            perf_artifacts = [a for a in artifacts if "performance" in a.artifact_type.lower()]
            return len(perf_artifacts) > 0

        elif criterion == "deployment_successful":
            # Check for deployment artifacts
            deploy_artifacts = [a for a in artifacts if "deployment" in a.artifact_type.lower()]
            return len(deploy_artifacts) > 0

        elif criterion == "monitoring_configured":
            # Check for monitoring configuration artifacts
            monitor_artifacts = [a for a in artifacts if "monitoring" in a.artifact_type.lower()]
            return len(monitor_artifacts) > 0

        elif criterion == "documentation_complete":
            # Check for documentation artifacts
            doc_artifacts = [a for a in artifacts if "documentation" in a.artifact_type.lower()]
            return len(doc_artifacts) > 0

        return False

    def transition_to_next_phase(self, project_id: UUID) -> Dict[str, Any]:
        """
        Transition project to the next SDLC phase if current phase is complete.

        Args:
            project_id: UUID of the project

        Returns:
            Dict with transition results
        """
        current_phase = self.get_current_phase(project_id)

        # Validate current phase completion
        validation = self.validate_phase_completion(project_id, current_phase)

        if not validation["is_complete"]:
            return {
                "transitioned": False,
                "error": f"Current phase '{current_phase}' is not complete",
                "validation": validation
            }

        # Get next phase
        phase_config = SDLC_PHASES[current_phase]
        next_phase = phase_config.get("next_phase")

        if not next_phase:
            return {
                "transitioned": False,
                "message": f"Project has completed final phase: {current_phase}",
                "project_complete": True
            }

        # Transition to next phase
        self.set_current_phase(project_id, next_phase)

        # Create phase transition artifact
        transition_artifact = self.context_store.create_artifact(
            project_id=project_id,
            source_agent="orchestrator",
            artifact_type="phase_transition",
            content={
                "from_phase": current_phase,
                "to_phase": next_phase,
                "transition_time": datetime.now(timezone.utc).isoformat(),
                "completion_validation": validation
            }
        )

        # Emit WebSocket event for phase transition
        event = WebSocketEvent(
            event_type=EventType.WORKFLOW_EVENT,
            project_id=project_id,
            data={
                "event": "phase_transition",
                "from_phase": current_phase,
                "to_phase": next_phase,
                "transition_artifact_id": str(transition_artifact.context_id)
            }
        )

        logger.info("Project transitioned to next phase",
                   project_id=project_id,
                   from_phase=current_phase,
                   to_phase=next_phase)

        return {
            "transitioned": True,
            "from_phase": current_phase,
            "to_phase": next_phase,
            "transition_artifact_id": str(transition_artifact.context_id)
        }

    def get_selective_context(self, project_id: UUID, phase: str, agent_type: str) -> List[UUID]:
        """
        Get selective context artifacts relevant to the current phase and agent.
        Enhanced with advanced context granularity features.

        Args:
            project_id: UUID of the project
            phase: Current SDLC phase
            agent_type: Type of agent requesting context

        Returns:
            List of relevant context artifact IDs
        """
        # Use the enhanced context store selective context method
        selective_result = self.context_store.get_selective_context(
            project_id=project_id,
            agent_type=agent_type,
            phase=phase
        )

        logger.info("Enhanced selective context retrieved",
                   project_id=project_id,
                   phase=phase,
                   agent_type=agent_type,
                   total_artifacts=selective_result["statistics"]["total_artifacts"],
                   selected_artifacts=selective_result["statistics"]["selected_count"],
                   reduction_percentage=selective_result["statistics"]["reduction_percentage"])

        return selective_result["context_ids"]

    def _is_artifact_relevant_to_agent(self, artifact, agent_type: str, phase: str) -> bool:
        """Determine if an artifact is relevant to a specific agent in a phase."""
        artifact_type = artifact.artifact_type.lower()

        # Analyst relevance
        if agent_type == "analyst":
            return any(keyword in artifact_type for keyword in ["user_input", "requirement", "analysis"])

        # Architect relevance
        elif agent_type == "architect":
            return any(keyword in artifact_type for keyword in ["requirement", "architecture", "design", "api", "model"])

        # Coder relevance
        elif agent_type == "coder":
            return any(keyword in artifact_type for keyword in ["architecture", "design", "api", "model", "spec"])

        # Tester relevance
        elif agent_type == "tester":
            return any(keyword in artifact_type for keyword in ["code", "source", "test", "spec", "requirement"])

        # Deployer relevance
        elif agent_type == "deployer":
            return any(keyword in artifact_type for keyword in ["code", "source", "deployment", "config"])

        return False

    def get_phase_progress(self, project_id: UUID) -> Dict[str, Any]:
        """Get comprehensive progress information for all phases."""
        current_phase = self.get_current_phase(project_id)
        all_phases = list(SDLC_PHASES.keys())
        current_index = all_phases.index(current_phase)

        phase_progress = []
        total_completion = 0

        for i, phase_name in enumerate(all_phases):
            phase_config = SDLC_PHASES[phase_name]
            validation = self.validate_phase_completion(project_id, phase_name)

            phase_info = {
                "phase": phase_name,
                "name": phase_config["name"],
                "description": phase_config["description"],
                "status": "pending",
                "completion_percentage": validation.get("completion_percentage", 0),
                "is_current": phase_name == current_phase,
                "is_completed": False,
                "estimated_duration_hours": phase_config["estimated_duration_hours"]
            }

            if i < current_index:
                phase_info["status"] = "completed"
                phase_info["is_completed"] = True
            elif i == current_index:
                phase_info["status"] = "in_progress"
            else:
                phase_info["status"] = "pending"

            phase_progress.append(phase_info)
            if phase_info["is_completed"]:
                total_completion += 100
            elif phase_info["is_current"]:
                total_completion += phase_info["completion_percentage"]

        overall_completion = total_completion / len(all_phases)

        return {
            "current_phase": current_phase,
            "current_phase_index": current_index,
            "total_phases": len(all_phases),
            "overall_completion_percentage": overall_completion,
            "phase_progress": phase_progress,
            "estimated_remaining_hours": sum(
                SDLC_PHASES[phase]["estimated_duration_hours"]
                for phase in all_phases[current_index:]
            )
        }

    async def run_project_workflow(self, project_id: UUID, user_idea: str, workflow_id: str = "greenfield-fullstack"):
        """
        Runs a dynamic workflow for a project using the WorkflowExecutionEngine.

        Args:
            project_id: UUID of the project
            user_idea: User's project idea/description
            workflow_id: ID of the workflow to execute (defaults to greenfield-fullstack)
        """
        logger.info("Starting dynamic project workflow",
                   project_id=project_id,
                   workflow_id=workflow_id,
                   user_idea=user_idea[:100])

        try:
            # Create initial context artifact with user idea
            initial_context = {
                "user_idea": user_idea,
                "project_id": str(project_id),
                "workflow_id": workflow_id
            }

            context_artifact = self.context_store.create_artifact(
                project_id=project_id,
                source_agent=AgentType.ORCHESTRATOR.value,
                artifact_type="user_input",
                content=initial_context
            )

            # Start workflow execution
            execution = await self.workflow_engine.start_workflow_execution(
                workflow_id=workflow_id,
                project_id=str(project_id),
                context_data=initial_context
            )

            logger.info("Workflow execution started",
                       execution_id=execution.execution_id,
                       workflow_id=workflow_id,
                       total_steps=execution.total_steps)

            # Execute workflow steps sequentially
            while not execution.is_complete():
                try:
                    # Execute next step
                    result = await self.workflow_engine.execute_workflow_step(execution.execution_id)

                    if result.get("status") == "no_pending_steps":
                        break

                    logger.info("Workflow step completed",
                               execution_id=execution.execution_id,
                               step_index=result.get("step_index"),
                               agent=result.get("agent"))

                    # Check for HITL requirements in step results
                    if result.get("requires_hitl"):
                        hitl_result = await self._handle_workflow_hitl(
                            execution.execution_id,
                            result
                        )
                        if not hitl_result.get("approved", True):
                            logger.warning("Workflow paused for HITL approval",
                                         execution_id=execution.execution_id)
                            break

                except Exception as e:
                    logger.error("Workflow step execution failed",
                               execution_id=execution.execution_id,
                               error=str(e))

                    # Mark execution as failed
                    await self.workflow_engine.cancel_workflow_execution(
                        execution.execution_id,
                        f"Step execution failed: {str(e)}"
                    )
                    raise

            # Check final status
            final_status = self.workflow_engine.get_workflow_execution_status(execution.execution_id)
            if final_status and final_status.get("is_complete"):
                if final_status.get("status") == "completed":
                    logger.info("Workflow execution completed successfully",
                               execution_id=execution.execution_id,
                               total_steps=final_status.get("total_steps"))
                else:
                    logger.warning("Workflow execution completed with failures",
                                 execution_id=execution.execution_id,
                                 failed_steps=final_status.get("failed_steps"))
            else:
                logger.warning("Workflow execution did not complete",
                             execution_id=execution.execution_id)

        except Exception as e:
            logger.error("Project workflow execution failed",
                        project_id=project_id,
                        workflow_id=workflow_id,
                        error=str(e))
            raise

    async def detect_and_resolve_conflicts(self, project_id: UUID, workflow_id: str) -> Dict[str, Any]:
        """
        Detect and attempt to resolve conflicts in a project workflow.

        Args:
            project_id: UUID of the project
            workflow_id: ID of the workflow

        Returns:
            Dictionary with conflict detection and resolution results
        """
        logger.info("Starting conflict detection and resolution",
                   project_id=project_id,
                   workflow_id=workflow_id)

        try:
            # Get project artifacts and tasks
            artifacts = self.context_store.get_artifacts_by_project(project_id)
            tasks = self.get_project_tasks(project_id)

            # Detect conflicts
            detected_conflicts = await self.conflict_resolver.detect_conflicts(
                str(project_id), workflow_id, artifacts, tasks
            )

            resolution_results = []
            successful_resolutions = 0
            escalated_conflicts = 0

            # Attempt to resolve each conflict
            for conflict in detected_conflicts:
                try:
                    # Attempt automatic resolution
                    resolution_result = await self.conflict_resolver.resolve_conflict(conflict.conflict_id)

                    if resolution_result.success:
                        successful_resolutions += 1
                        logger.info("Conflict resolved successfully",
                                   conflict_id=conflict.conflict_id,
                                   strategy=resolution_result.resolution_strategy.value)
                    else:
                        escalated_conflicts += 1
                        logger.warning("Conflict escalated for manual resolution",
                                     conflict_id=conflict.conflict_id,
                                     reason=resolution_result.resolution_details.get("escalation_reason", "Unknown"))

                    resolution_results.append({
                        "conflict_id": conflict.conflict_id,
                        "type": conflict.conflict_type.value,
                        "severity": conflict.severity.value,
                        "resolved": resolution_result.success,
                        "strategy": resolution_result.resolution_strategy.value if resolution_result.success else None,
                        "escalated": not resolution_result.success
                    })

                except Exception as e:
                    logger.error("Failed to resolve conflict",
                               conflict_id=conflict.conflict_id,
                               error=str(e))
                    escalated_conflicts += 1

                    resolution_results.append({
                        "conflict_id": conflict.conflict_id,
                        "type": conflict.conflict_type.value,
                        "severity": conflict.severity.value,
                        "resolved": False,
                        "strategy": None,
                        "escalated": True,
                        "error": str(e)
                    })

            # Get conflict statistics
            conflict_stats = self.conflict_resolver.get_conflict_statistics(str(project_id))

            result = {
                "project_id": str(project_id),
                "workflow_id": workflow_id,
                "conflicts_detected": len(detected_conflicts),
                "successful_resolutions": successful_resolutions,
                "escalated_conflicts": escalated_conflicts,
                "resolution_rate": (successful_resolutions / len(detected_conflicts)) * 100 if detected_conflicts else 100,
                "conflict_details": resolution_results,
                "conflict_statistics": conflict_stats,
                "conflict_patterns": self.conflict_resolver.get_conflict_patterns_report()
            }

            logger.info("Conflict detection and resolution completed",
                       project_id=project_id,
                       detected=len(detected_conflicts),
                       resolved=successful_resolutions,
                       escalated=escalated_conflicts)

            return result

        except Exception as e:
            logger.error("Conflict detection and resolution failed",
                        project_id=project_id,
                        workflow_id=workflow_id,
                        error=str(e))
            return {
                "project_id": str(project_id),
                "workflow_id": workflow_id,
                "error": str(e),
                "conflicts_detected": 0,
                "successful_resolutions": 0,
                "escalated_conflicts": 0
            }

    async def execute_sdlc_workflow(self, project_id: UUID, user_idea: str) -> Dict[str, Any]:
        """
        Execute a complete 6-phase SDLC workflow for a project.

        This method orchestrates the full Software Development Life Cycle from
        Discovery through Launch, with proper phase gates and quality controls.

        Args:
            project_id: UUID of the project
            user_idea: User's project idea/description

        Returns:
            Dictionary with complete SDLC execution results
        """
        from app.models.workflow import SDLCWorkflowDefinition

        logger.info("Starting SDLC workflow execution",
                   project_id=project_id,
                   user_idea=user_idea[:100])

        try:
            # Create initial context artifact with user idea
            initial_context = {
                "user_idea": user_idea,
                "project_id": str(project_id),
                "workflow_type": "sdlc"
            }

            context_artifact = self.context_store.create_artifact(
                project_id=project_id,
                source_agent=AgentType.ORCHESTRATOR.value,
                artifact_type="user_input",
                content=initial_context
            )

            # Execute SDLC workflow using the enhanced workflow engine
            sdlc_result = await self.workflow_engine.execute_sdlc_workflow(
                project_id=str(project_id),
                workflow_id="sdlc-workflow",
                context_data=initial_context
            )

            # Log SDLC completion results
            if sdlc_result.get("status") == "completed":
                logger.info("SDLC workflow completed successfully",
                           project_id=project_id,
                           total_phases=sdlc_result.get("total_phases"),
                           execution_id=sdlc_result.get("execution_id"))

                # Update project phase to completed
                self.set_current_phase(project_id, "completed")

            elif sdlc_result.get("status") == "paused_for_hitl":
                logger.info("SDLC workflow paused for HITL approval",
                           project_id=project_id,
                           hitl_request_id=sdlc_result.get("hitl_request_id"))

            elif sdlc_result.get("status") == "paused_for_review":
                logger.warning("SDLC workflow paused for manual review",
                             project_id=project_id,
                             issue=sdlc_result.get("issue"))

            elif sdlc_result.get("status") == "failed":
                logger.error("SDLC workflow failed",
                           project_id=project_id,
                           failed_phase=sdlc_result.get("failed_phase"),
                           error=sdlc_result.get("error"))

                # Update project phase to failed
                self.set_current_phase(project_id, "failed")

            return sdlc_result

        except Exception as e:
            logger.error("SDLC workflow execution failed",
                        project_id=project_id,
                        error=str(e))

            # Update project phase to failed
            self.set_current_phase(project_id, "failed")

            return {
                "status": "failed",
                "error": str(e),
                "project_id": str(project_id)
            }

    def run_project_workflow_sync(self, project_id: UUID, user_idea: str, workflow_id: str = "greenfield-fullstack"):
        """
        Synchronous wrapper for running project workflow (for testing purposes).

        This method provides a synchronous interface to the async workflow execution
        for use in test environments where async/await is not directly supported.
        """
        import asyncio

        async def _run():
            return await self.run_project_workflow(project_id, user_idea, workflow_id)

        # Create new event loop for testing
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If loop is already running, we need to handle differently
                try:
                    import nest_asyncio
                    nest_asyncio.apply()
                    return loop.run_until_complete(_run())
                except ImportError:
                    logger.warning("nest_asyncio not available, cannot run workflow in running event loop")
                    return None
            else:
                return loop.run_until_complete(_run())
        except RuntimeError:
            # No event loop, create new one
            return asyncio.run(_run())

    async def _handle_workflow_hitl(self, execution_id: str, step_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Handle HITL requirements during workflow execution.

        Args:
            execution_id: Workflow execution ID
            step_result: Result from workflow step execution

        Returns:
            HITL handling result
        """
        # Get execution details
        execution = self.workflow_engine._get_execution_state(execution_id)
        if not execution:
            return {"approved": False, "error": "Execution not found"}

        # Create HITL request based on step result
        question = step_result.get("hitl_question", "Please review and approve this step result")
        task_id = step_result.get("task_id")

        if task_id:
            hitl_request = self.create_hitl_request(
                project_id=UUID(execution.project_id),
                task_id=UUID(task_id),
                question=question,
                options=["Approve", "Reject", "Amend"]
            )

            # Pause workflow execution immediately
            await self.workflow_engine.pause_workflow_execution(
                execution_id,
                f"HITL approval required for step {step_result.get('step_index')}: {question}"
            )

            # Update agent status to waiting for HITL
            agent_type = step_result.get("agent")
            if agent_type:
                self.update_agent_status(agent_type, AgentStatus.WAITING_FOR_HITL, UUID(task_id))

            # Emit WebSocket event for HITL request
            await self.notify_hitl_request_created(hitl_request.id)

            logger.info("Workflow paused for HITL approval",
                       execution_id=execution_id,
                       hitl_request_id=str(hitl_request.id),
                       agent=agent_type,
                       step_index=step_result.get("step_index"))

            return {
                "approved": False,
                "paused": True,
                "hitl_request_id": str(hitl_request.id),
                "reason": "HITL approval required"
            }

        return {"approved": True}

    async def wait_for_hitl_response(self, request_id: UUID):
        # In a real app, this would involve waiting for an external event.
        # For this simulation, we can check the status in a loop with a timeout.
        # This is a simplified placeholder.
        pass

    def create_hitl_request(
        self,
        project_id: UUID,
        task_id: UUID,
        question: str,
        options: Optional[List[str]] = None,
        ttl_hours: Optional[int] = None
    ) -> HitlRequestDB:
        """Create a new HITL request with optional TTL."""

        # Validation
        if not question or question.strip() == "":
            raise ValueError("Question cannot be empty")

        # Validate that task belongs to the specified project
        task = self.db.query(TaskDB).filter(TaskDB.id == task_id).first()
        if not task:
            raise ValueError(f"Task {task_id} not found")

        if task.project_id != project_id:
            raise ValueError(f"Task {task_id} does not belong to project {project_id}")

        if options is None:
            options = []
            
        # Calculate expiration time if TTL is provided
        expires_at = None
        expiration_time = None
        if ttl_hours is not None:
            expires_at = datetime.now(timezone.utc) + timedelta(hours=ttl_hours)
            expiration_time = expires_at
        
        hitl_request = HitlRequestDB(
            project_id=project_id,
            task_id=task_id,
            question=question,
            options=options,
            status=HitlStatus.PENDING,
            expires_at=expires_at,
            expiration_time=expiration_time
        )
        self.db.add(hitl_request)
        self.db.commit()
        self.db.refresh(hitl_request)
        logger.info("HITL request created", hitl_request_id=hitl_request.id, ttl_hours=ttl_hours)
        return hitl_request
    
    def process_hitl_response(
        self,
        hitl_request_id: UUID,
        action: str,
        comment: Optional[str] = None,
        amended_data: Optional[Dict[str, Any]] = None,
        user_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """Process a HITL response (approve, reject, or amend) and resume workflow if needed."""

        # Get the HITL request
        hitl_request = self.db.query(HitlRequestDB).filter(
            HitlRequestDB.id == hitl_request_id
        ).first()

        if not hitl_request:
            raise ValueError(f"HITL request {hitl_request_id} not found")

        if hitl_request.status != HitlStatus.PENDING:
            raise ValueError(f"HITL request {hitl_request_id} is not in pending status")


        # Update the request based on action
        response_data = {"action": action, "workflow_resumed": False}

        if action == "approve":
            hitl_request.status = HitlStatus.APPROVED
            hitl_request.user_response = "approved"

        elif action == "reject":
            hitl_request.status = HitlStatus.REJECTED
            hitl_request.user_response = "rejected"

        elif action == "amend":
            hitl_request.status = HitlStatus.AMENDED
            hitl_request.user_response = "amended"
            if amended_data:
                hitl_request.amended_content = amended_data

        else:
            raise ValueError(f"Invalid action: {action}")

        # Common updates
        hitl_request.response_comment = comment
        hitl_request.responded_at = datetime.now(timezone.utc)

        # Add to history
        history_entry = {
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "action": action,
            "user_id": user_id,
            "comment": comment,
            "amended_data": amended_data
        }

        if hitl_request.history is None:
            hitl_request.history = []
        hitl_request.history.append(history_entry)

        self.db.commit()
        self.db.refresh(hitl_request)

        logger.info("HITL response processed",
                   hitl_request_id=hitl_request_id,
                   action=action,
                   new_status=hitl_request.status)

        # Attempt to resume workflow after HITL response
        try:
            resume_result = self._resume_workflow_after_hitl(hitl_request, action)
            response_data.update(resume_result)
        except Exception as e:
            logger.error("Failed to resume workflow after HITL",
                        hitl_request_id=hitl_request_id,
                        error=str(e))
            response_data["workflow_resume_error"] = str(e)

        return response_data

    def _resume_workflow_after_hitl(self, hitl_request: HitlRequestDB, action: str) -> Dict[str, Any]:
        """
        Resume workflow execution after HITL response.

        Args:
            hitl_request: The processed HITL request
            action: The HITL action taken (approve/reject/amend)

        Returns:
            Dictionary with workflow resume results
        """
        # Get the task associated with this HITL request
        task = self.db.query(TaskDB).filter(TaskDB.id == hitl_request.task_id).first()
        if not task:
            logger.warning("Task not found for HITL resume", task_id=hitl_request.task_id)
            return {"workflow_resumed": False, "error": "Associated task not found"}

        # Update agent status based on HITL action
        if action == "approve":
            # Agent can continue with approved work
            self.update_agent_status(task.agent_type, AgentStatus.IDLE)
            task_status = TaskStatus.COMPLETED
        elif action == "reject":
            # Agent work was rejected, mark as failed
            self.update_agent_status(task.agent_type, AgentStatus.IDLE)
            task_status = TaskStatus.FAILED
            task.error_message = f"Task rejected via HITL: {hitl_request.response_comment or 'No comment provided'}"
        elif action == "amend":
            # Agent needs to incorporate amendments
            self.update_agent_status(task.agent_type, AgentStatus.IDLE)
            task_status = TaskStatus.COMPLETED
            # Store amended content as task output
            if hitl_request.amended_content:
                task.output = hitl_request.amended_content

        # Update task status
        self.update_task_status(task.task_id, task_status)

        # Try to find and resume any paused workflows
        workflow_resumed = False
        execution_id = None

        try:
            # Look for workflow executions that might be paused for this task
            # This is a simplified approach - in production, you'd have better tracking
            from app.database.models import WorkflowStateDB
            from app.models.workflow_state import WorkflowExecutionState as ExecutionStateEnum

            paused_workflows = self.db.query(WorkflowStateDB).filter(
                WorkflowStateDB.project_id == hitl_request.project_id,
                WorkflowStateDB.status == ExecutionStateEnum.PAUSED.value
            ).all()

            for workflow_state in paused_workflows:
                # Check if this workflow contains the task that was waiting for HITL
                if workflow_state.steps_data:
                    for step_data in workflow_state.steps_data:
                        if step_data.get("task_id") == str(hitl_request.task_id):
                            # Found the workflow, attempt to resume it
                            execution_id = workflow_state.execution_id

                            # Resume the workflow
                            resume_success = self.workflow_engine.resume_workflow_execution_sync(execution_id)
                            if resume_success:
                                workflow_resumed = True
                                logger.info("Workflow resumed after HITL response",
                                           execution_id=execution_id,
                                           hitl_request_id=str(hitl_request.id),
                                           action=action)
                            break

                if workflow_resumed:
                    break

        except Exception as e:
            logger.error("Error resuming workflow after HITL",
                        hitl_request_id=str(hitl_request.id),
                        error=str(e))

        # Emit workflow resume event
        event = WebSocketEvent(
            event_type=EventType.WORKFLOW_RESUMED,
            project_id=hitl_request.project_id,
            task_id=hitl_request.task_id,
            data={
                "message": f"Workflow resumed after HITL {action}",
                "task_id": str(hitl_request.task_id),
                "agent_type": task.agent_type,
                "hitl_action": action,
                "execution_id": execution_id
            }
        )

        logger.info("HITL response processing completed",
                   hitl_request_id=str(hitl_request.id),
                   action=action,
                   workflow_resumed=workflow_resumed,
                   task_status=task_status.value)

        return {
            "workflow_resumed": workflow_resumed,
            "task_status": task_status.value,
            "execution_id": execution_id
        }

    def create_project(self, name: str, description: str = None) -> UUID:
        """Create a new project."""
        
        project = ProjectDB(
            name=name,
            description=description,
            status="active"
        )
        
        self.db.add(project)
        self.db.commit()
        self.db.refresh(project)
        
        logger.info("Project created", project_id=project.id, name=name)
        
        return project.id
    
    def create_task(
        self,
        project_id: UUID,
        agent_type: str,
        instructions: str,
        context_ids: List[UUID] = None
    ) -> Task:
        """Create a new task for an agent."""
        
        if context_ids is None:
            context_ids = []
        
        # Convert UUIDs to strings for JSON serialization
        context_ids_str = [str(cid) for cid in context_ids]
        
        db_task = TaskDB(
            project_id=project_id,
            agent_type=agent_type,
            instructions=instructions,
            context_ids=context_ids_str
        )
        
        self.db.add(db_task)
        self.db.commit()
        self.db.refresh(db_task)
        
        # Convert context_ids back to UUIDs for the Pydantic model
        context_ids_uuid = [UUID(cid) if isinstance(cid, str) else cid for cid in db_task.context_ids]
        
        task = Task(
            task_id=db_task.id,
            project_id=db_task.project_id,
            agent_type=db_task.agent_type,
            status=db_task.status,
            context_ids=context_ids_uuid,
            instructions=db_task.instructions,
            output=db_task.output,
            error_message=db_task.error_message,
            created_at=db_task.created_at,
            updated_at=db_task.updated_at,
            started_at=db_task.started_at,
            completed_at=db_task.completed_at
        )
        
        logger.info("Task created", 
                   task_id=task.task_id, 
                   agent_type=agent_type,
                   project_id=project_id)
        
        return task
    
    def submit_task(self, task: Task) -> str:
        """Submit a task to the Celery queue."""

        # Import here to avoid circular import
        from app.tasks.agent_tasks import process_agent_task

        task_data = {
            "task_id": str(task.task_id),
            "project_id": str(task.project_id),
            "agent_type": task.agent_type,
            "instructions": task.instructions,
            "context_ids": [str(cid) for cid in task.context_ids]
        }

        # Submit to Celery
        celery_task = process_agent_task.delay(task_data)
        
        # Update task status to working
        self.update_task_status(task.task_id, TaskStatus.WORKING)
        
        # Update agent status
        self.update_agent_status(task.agent_type, AgentStatus.WORKING, task.task_id)
        
        # Emit WebSocket event
        event = WebSocketEvent(
            event_type=EventType.TASK_STARTED,
            project_id=task.project_id,
            task_id=task.task_id,
            agent_type=task.agent_type,
            data={
                "status": "working",
                "celery_task_id": celery_task.id
            }
        )
        
        # Note: In a real implementation, we would use asyncio to send the event
        logger.info("Task submitted to queue", 
                   task_id=task.task_id,
                   celery_task_id=celery_task.id)
        
        return celery_task.id
    
    def update_task_status(
        self,
        task_id: UUID,
        status: TaskStatus,
        output: Dict[str, Any] = None,
        error_message: str = None
    ):
        """Update a task's status."""
        
        db_task = self.db.query(TaskDB).filter(TaskDB.id == task_id).first()
        
        if not db_task:
            logger.error("Task not found", task_id=task_id)
            return
        
        db_task.status = status
        
        if output is not None:
            db_task.output = output
        
        if error_message is not None:
            db_task.error_message = error_message
        
        if status == TaskStatus.WORKING and db_task.started_at is None:
                db_task.started_at = datetime.now(timezone.utc)
        
        if status in [TaskStatus.COMPLETED, TaskStatus.FAILED, TaskStatus.CANCELLED]:
                db_task.completed_at = datetime.now(timezone.utc)
        
        self.db.commit()
        
        logger.info("Task status updated", 
                   task_id=task_id, 
                   status=status)
    
    def update_agent_status(
        self,
        agent_type: str,
        status: AgentStatus,
        current_task_id: UUID = None,
        error_message: str = None
    ):
        """Update an agent's status."""
        
        db_agent = self.db.query(AgentStatusDB).filter(
            AgentStatusDB.agent_type == agent_type
        ).first()
        
        if not db_agent:
            # Create new agent status record
            db_agent = AgentStatusDB(
                agent_type=agent_type,
                status=status,
                current_task_id=current_task_id,
                error_message=error_message
            )
            self.db.add(db_agent)
        else:
            db_agent.status = status
            db_agent.current_task_id = current_task_id
            if error_message is not None:
                db_agent.error_message = error_message
        
        self.db.commit()
        
        logger.info("Agent status updated", 
                   agent_type=agent_type, 
                   status=status)
    
    def get_agent_status(self, agent_type: str) -> Optional[AgentStatus]:
        """Get an agent's current status."""
        
        db_agent = self.db.query(AgentStatusDB).filter(
            AgentStatusDB.agent_type == agent_type
        ).first()
        
        if not db_agent:
            return None
        
        return db_agent.status
    
    def get_project_tasks(self, project_id: UUID) -> List[Task]:
        """Get all tasks for a project."""
        
        db_tasks = self.db.query(TaskDB).filter(
            TaskDB.project_id == project_id
        ).all()
        
        tasks = []
        for db_task in db_tasks:
            # Convert context_ids back to UUIDs for the Pydantic model
            context_ids_uuid = [UUID(cid) if isinstance(cid, str) else cid for cid in db_task.context_ids]
            
            task = Task(
                task_id=db_task.id,
                project_id=db_task.project_id,
                agent_type=db_task.agent_type,
                status=db_task.status,
                context_ids=context_ids_uuid,
                instructions=db_task.instructions,
                output=db_task.output,
                error_message=db_task.error_message,
                created_at=db_task.created_at,
                updated_at=db_task.updated_at,
                started_at=db_task.started_at,
                completed_at=db_task.completed_at
            )
            tasks.append(task)
        
        return tasks
    
    def create_task_from_handoff(
        self, 
        handoff: HandoffSchema = None,
        project_id: UUID = None, 
        handoff_schema: Dict[str, Any] = None
    ) -> Task:
        """Create a task from a HandoffSchema or raw handoff data."""
        
        if handoff is not None:
            # Original interface - HandoffSchema object
            task = self.create_task(
                project_id=handoff.project_id,
                agent_type=handoff.to_agent,
                instructions=handoff.instructions,
                context_ids=handoff.context_ids
            )
        elif project_id is not None and handoff_schema is not None:
            # New interface - raw dictionary data
            from uuid import UUID as UUID_type
            context_ids_uuid = []
            if handoff_schema.get("context_ids"):
                context_ids_uuid = [
                    UUID_type(cid) if isinstance(cid, str) else cid 
                    for cid in handoff_schema["context_ids"]
                ]
            
            task = self.create_task(
                project_id=project_id,
                agent_type=handoff_schema["to_agent"],
                instructions=handoff_schema.get("task_instructions", handoff_schema.get("instructions", "")),
                context_ids=context_ids_uuid
            )
        else:
            raise ValueError("Either handoff object or (project_id, handoff_schema) must be provided")
        
        # Store the handoff metadata with the task - ensure all data is JSON serializable
        if handoff is not None:
            handoff_metadata = {
                "handoff_id": str(handoff.handoff_id),
                "from_agent": handoff.from_agent,
                "phase": handoff.phase,
                "expected_outputs": handoff.expected_outputs,
                "metadata": handoff.metadata
            }
            
            self.update_task_status(
                task.task_id, 
                TaskStatus.PENDING, 
                output=handoff_metadata
            )
            
            logger.info("Task created from handoff",
                       task_id=task.task_id,
                       handoff_id=handoff.handoff_id,
                       from_agent=handoff.from_agent,
                       to_agent=handoff.to_agent)
        else:
            # For raw handoff schema, store what we have
            handoff_metadata = {
                "handoff_source": "raw_schema",
                "from_agent": handoff_schema.get("from_agent"),
                "to_agent": handoff_schema.get("to_agent"),
                "expected_output": handoff_schema.get("expected_output"),
                "metadata": handoff_schema
            }
            
            self.update_task_status(
                task.task_id, 
                TaskStatus.PENDING, 
                output=handoff_metadata
            )
            
            logger.info("Task created from raw handoff schema",
                       task_id=task.task_id,
                       from_agent=handoff_schema.get("from_agent"),
                       to_agent=handoff_schema.get("to_agent"))
        
        return task
    
    async def process_task_with_autogen(self, task: Task, handoff: HandoffSchema) -> dict:
        """Process a task using the AutoGen framework."""
        
        # Get context artifacts for the task
        context_artifacts = self.context_store.get_artifacts_by_ids(task.context_ids)
        
        # Update agent status to working
        self.update_agent_status(task.agent_type, AgentStatus.WORKING, task.task_id)
        
        logger.info("Processing task with AutoGen", 
                   task_id=task.task_id, 
                   agent_type=task.agent_type)
        
        # Execute the task using AutoGen service
        result = await self.autogen_service.execute_task(task, handoff, context_artifacts)
        
        # Update agent status based on result
        if result.get("success", True):
            self.update_agent_status(task.agent_type, AgentStatus.IDLE)
        else:
            self.update_agent_status(task.agent_type, AgentStatus.ERROR, error_message=result.get("error"))
        
        return result
    
    async def handle_hitl_checkpoint(self, project_id: UUID, task_id: UUID, question: str, task_result: dict) -> str:
        """Handle a Human-in-the-Loop checkpoint."""
        
        hitl_request = self.create_hitl_request(project_id, task_id, question)
        
        # Add task result to HITL request for user review
        self.update_hitl_request_content(hitl_request.id, task_result)
        
        # Emit WebSocket event for real-time notification
        await self.notify_hitl_request_created(hitl_request.id)
        
        # Wait for user response (in real implementation, this would be event-driven)
        response = await self.wait_for_hitl_response(hitl_request.id)
        
        return response
    
    def update_hitl_request_content(self, request_id: UUID, content: dict):
        """Update HITL request with content for user review."""
        
        hitl_request = self.db.query(HitlRequestDB).filter(HitlRequestDB.id == request_id).first()
        if hitl_request:
            hitl_request.amended_content = content
            self.db.commit()
    
    async def notify_hitl_request_created(self, request_id: UUID):
        """Notify frontend of new HITL request via WebSocket."""
        
        event = WebSocketEvent(
            event_type=EventType.HITL_REQUEST_CREATED,
            data={"hitl_request_id": str(request_id)}
        )
        
        # In a full implementation, this would emit the event to connected WebSocket clients
        logger.info("HITL request notification sent", request_id=request_id)
    
    def get_latest_amended_artifact(self, project_id: UUID, task_id: UUID) -> Optional['ContextArtifact']:
        """Get the latest amended artifact for a task."""
        
        hitl_request = self.db.query(HitlRequestDB).filter(
            HitlRequestDB.task_id == task_id,
            HitlRequestDB.status == HitlStatus.AMENDED
        ).first()
        
        if hitl_request and hitl_request.amended_content:
            # Create new context artifact with amended content
            amended_artifact = self.context_store.create_artifact(
                project_id=project_id,
                source_agent="user_amendment",
                artifact_type="hitl_response",
                content=hitl_request.amended_content
            )
            return amended_artifact
        
        return None
    
    async def resume_workflow_after_hitl(self, hitl_request_id: UUID, hitl_action):
        """Resume workflow after HITL response."""

        logger.info("Resuming workflow after HITL response",
                   hitl_request_id=hitl_request_id,
                   hitl_action=hitl_action)

        # Get the HITL request to find the associated task and project
        hitl_request = self.db.query(HitlRequestDB).filter(
            HitlRequestDB.id == hitl_request_id
        ).first()

        if not hitl_request:
            logger.error("HITL request not found", hitl_request_id=hitl_request_id)
            return {
                "workflow_resumed": False,
                "error": "HITL request not found"
            }

        # Get the task that was waiting for HITL
        task = self.db.query(TaskDB).filter(TaskDB.id == hitl_request.task_id).first()
        if not task:
            logger.error("Task not found for HITL resume", task_id=hitl_request.task_id)
            return {
                "workflow_resumed": False,
                "error": "Associated task not found"
            }

        # Update agent status to idle (ready for next task)
        self.update_agent_status(task.agent_type, AgentStatus.IDLE)

        # Determine next action based on HITL action
        next_action = "continue_task"
        if hitl_action == "reject":
            next_action = "handle_rejection"
        elif hitl_action == "amend":
            next_action = "apply_amendments"

        # Emit workflow resume event
        event = WebSocketEvent(
            event_type=EventType.WORKFLOW_RESUMED,
            project_id=hitl_request.project_id,
            task_id=hitl_request.task_id,
            data={
                "message": "Workflow resumed after HITL response",
                "task_id": str(hitl_request.task_id),
                "agent_type": task.agent_type,
                "hitl_action": hitl_action,
                "next_action": next_action
            }
        )

        logger.info("Workflow resume event emitted",
                   project_id=hitl_request.project_id,
                   task_id=hitl_request.task_id,
                   hitl_action=hitl_action)

        return {
            "workflow_resumed": True,
            "next_action": next_action,
            "project_id": str(hitl_request.project_id),
            "task_id": str(hitl_request.task_id),
            "hitl_action": hitl_action
        }

    # ===== TIME-CONSCIOUS ORCHESTRATION METHODS =====

    def get_phase_time_analysis(self, project_id: UUID) -> Dict[str, Any]:
        """
        Analyze time performance for all phases of a project.

        Args:
            project_id: UUID of the project

        Returns:
            Comprehensive time analysis for all phases
        """
        current_phase = self.get_current_phase(project_id)
        all_phases = list(SDLC_PHASES.keys())

        phase_time_analysis = []
        total_actual_time = 0
        total_estimated_time = 0

        for phase_name in all_phases:
            phase_config = SDLC_PHASES[phase_name]
            phase_analysis = self._analyze_phase_time_performance(project_id, phase_name, phase_config)

            phase_time_analysis.append(phase_analysis)
            total_actual_time += phase_analysis.get("actual_duration_hours", 0)
            total_estimated_time += phase_config["estimated_duration_hours"]

        # Calculate overall time performance
        time_efficiency = (total_estimated_time / total_actual_time) * 100 if total_actual_time > 0 else 0
        time_variance = ((total_actual_time - total_estimated_time) / total_estimated_time) * 100 if total_estimated_time > 0 else 0

        return {
            "project_id": str(project_id),
            "current_phase": current_phase,
            "total_estimated_hours": total_estimated_time,
            "total_actual_hours": total_actual_time,
            "time_efficiency_percentage": time_efficiency,
            "time_variance_percentage": time_variance,
            "phase_time_analysis": phase_time_analysis,
            "time_based_recommendations": self._generate_time_based_recommendations(phase_time_analysis, current_phase)
        }

    def _analyze_phase_time_performance(self, project_id: UUID, phase_name: str, phase_config: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analyze time performance for a specific phase.

        Args:
            project_id: UUID of the project
            phase_name: Name of the phase
            phase_config: Phase configuration

        Returns:
            Time performance analysis for the phase
        """
        # Get tasks for this phase
        project_tasks = self.get_project_tasks(project_id)
        phase_tasks = [task for task in project_tasks if task.agent_type in phase_config["agent_sequence"]]

        # Calculate phase duration
        phase_start_time = None
        phase_end_time = None

        for task in phase_tasks:
            if task.started_at:
                if phase_start_time is None or task.started_at < phase_start_time:
                    phase_start_time = task.started_at

            if task.completed_at:
                if phase_end_time is None or task.completed_at > phase_end_time:
                    phase_end_time = task.completed_at

        # Calculate actual duration
        actual_duration_hours = 0
        if phase_start_time and phase_end_time:
            duration = phase_end_time - phase_start_time
            actual_duration_hours = duration.total_seconds() / 3600

        # Calculate efficiency metrics
        estimated_hours = phase_config["estimated_duration_hours"]
        max_hours = phase_config["max_duration_hours"]
        efficiency = (estimated_hours / actual_duration_hours) * 100 if actual_duration_hours > 0 else 0

        # Determine time status
        time_status = "on_track"
        if actual_duration_hours > max_hours:
            time_status = "overtime"
        elif actual_duration_hours > estimated_hours * phase_config["time_pressure_threshold"]:
            time_status = "warning"

        # Check for time-based decisions
        time_based_decisions = []
        if time_status == "warning":
            time_based_decisions.append(phase_config["time_based_decisions"]["overtime_action"])
        elif efficiency > 120:  # More than 20% faster than estimated
            time_based_decisions.append(phase_config["time_based_decisions"]["efficiency_bonus"])

        return {
            "phase": phase_name,
            "estimated_duration_hours": estimated_hours,
            "max_duration_hours": max_hours,
            "actual_duration_hours": actual_duration_hours,
            "efficiency_percentage": efficiency,
            "time_status": time_status,
            "time_based_decisions": time_based_decisions,
            "task_count": len(phase_tasks),
            "completed_tasks": len([t for t in phase_tasks if t.status == TaskStatus.COMPLETED]),
            "phase_start_time": phase_start_time.isoformat() if phase_start_time else None,
            "phase_end_time": phase_end_time.isoformat() if phase_end_time else None
        }

    def _generate_time_based_recommendations(self, phase_time_analysis: List[Dict[str, Any]], current_phase: str) -> List[str]:
        """
        Generate time-based recommendations based on phase analysis.

        Args:
            phase_time_analysis: Time analysis for all phases
            current_phase: Current active phase

        Returns:
            List of time-based recommendations
        """
        recommendations = []

        # Analyze current phase performance
        current_phase_analysis = next((p for p in phase_time_analysis if p["phase"] == current_phase), None)
        if current_phase_analysis:
            if current_phase_analysis["time_status"] == "warning":
                recommendations.append(f"Consider {current_phase_analysis['time_based_decisions'][0]} for {current_phase} phase")
            elif current_phase_analysis["time_status"] == "overtime":
                recommendations.append(f"Escalate {current_phase} phase - exceeded maximum duration")

        # Analyze overall project performance
        overtime_phases = [p for p in phase_time_analysis if p["time_status"] == "overtime"]
        if overtime_phases:
            recommendations.append(f"Review time management for {len(overtime_phases)} phases that exceeded time limits")

        # Analyze efficiency patterns
        high_efficiency_phases = [p for p in phase_time_analysis if p["efficiency_percentage"] > 120]
        if high_efficiency_phases:
            recommendations.append(f"Consider applying efficiency improvements from {len(high_efficiency_phases)} high-performing phases")

        return recommendations

    def get_time_conscious_context(self, project_id: UUID, phase: str, agent_type: str, time_budget_hours: float = None) -> Dict[str, Any]:
        """
        Get time-conscious context with selective injection based on time constraints.

        Args:
            project_id: UUID of the project
            phase: Current SDLC phase
            agent_type: Type of agent requesting context
            time_budget_hours: Available time budget in hours

        Returns:
            Time-conscious context with selective artifacts and time-based instructions
        """
        # Get base selective context
        base_context_ids = self.get_selective_context(project_id, phase, agent_type)

        # Get time analysis for current phase
        time_analysis = self.get_phase_time_analysis(project_id)
        current_phase_analysis = next(
            (p for p in time_analysis["phase_time_analysis"] if p["phase"] == phase),
            None
        )

        # Determine time pressure level
        time_pressure = "normal"
        if current_phase_analysis:
            if current_phase_analysis["time_status"] == "warning":
                time_pressure = "high"
            elif current_phase_analysis["time_status"] == "overtime":
                time_pressure = "critical"

        # Apply time-based context filtering
        filtered_context_ids = self._apply_time_based_context_filtering(
            base_context_ids, time_pressure, time_budget_hours, agent_type
        )

        # Generate time-based instructions
        time_instructions = self._generate_time_based_instructions(
            time_pressure, time_budget_hours, agent_type, current_phase_analysis
        )

        return {
            "context_ids": filtered_context_ids,
            "time_pressure": time_pressure,
            "time_budget_hours": time_budget_hours,
            "time_instructions": time_instructions,
            "time_analysis": current_phase_analysis,
            "context_reduction_percentage": ((len(base_context_ids) - len(filtered_context_ids)) / len(base_context_ids)) * 100 if base_context_ids else 0
        }

    def _apply_time_based_context_filtering(self, context_ids: List[UUID], time_pressure: str,
                                           time_budget: float, agent_type: str) -> List[UUID]:
        """
        Apply time-based filtering to context artifacts.

        Args:
            context_ids: Original context artifact IDs
            time_pressure: Time pressure level (normal/high/critical)
            time_budget: Available time budget in hours
            agent_type: Type of agent

        Returns:
            Filtered context artifact IDs
        """
        if time_pressure == "normal":
            return context_ids  # No filtering needed

        # Get context artifacts for analysis
        context_artifacts = self.context_store.get_artifacts_by_ids(context_ids)

        # Apply filtering based on time pressure
        filtered_ids = []

        for artifact in context_artifacts:
            # Always include critical artifacts
            if self._is_critical_artifact(artifact, agent_type):
                filtered_ids.append(artifact.context_id)
                continue

            # Apply time-based filtering
            if time_pressure == "high":
                # Include only recent and high-priority artifacts
                if self._is_recent_artifact(artifact) or self._is_high_priority_artifact(artifact, agent_type):
                    filtered_ids.append(artifact.context_id)
            elif time_pressure == "critical":
                # Include only most critical artifacts
                if self._is_high_priority_artifact(artifact, agent_type):
                    filtered_ids.append(artifact.context_id)

        logger.info("Applied time-based context filtering",
                   original_count=len(context_ids),
                   filtered_count=len(filtered_ids),
                   time_pressure=time_pressure,
                   agent_type=agent_type)

        return filtered_ids

    def _is_critical_artifact(self, artifact, agent_type: str) -> bool:
        """Determine if an artifact is critical for the agent."""
        critical_types = {
            "analyst": ["user_input", "requirements"],
            "architect": ["requirements", "architecture"],
            "coder": ["architecture", "design", "api"],
            "tester": ["code", "test", "requirements"],
            "deployer": ["code", "deployment", "config"]
        }

        return artifact.artifact_type in critical_types.get(agent_type, [])

    def _is_recent_artifact(self, artifact) -> bool:
        """Determine if an artifact is recent (created within last 24 hours)."""
        if not hasattr(artifact, 'created_at') or not artifact.created_at:
            return False

        time_diff = datetime.now(timezone.utc) - artifact.created_at
        return time_diff.total_seconds() < 86400  # 24 hours

    def _is_high_priority_artifact(self, artifact, agent_type: str) -> bool:
        """Determine if an artifact is high priority for the agent."""
        high_priority_types = {
            "analyst": ["user_input", "analysis", "requirements"],
            "architect": ["architecture", "design", "api", "model"],
            "coder": ["source_code", "design", "api"],
            "tester": ["source_code", "test", "quality"],
            "deployer": ["deployment", "config", "monitoring"]
        }

        return artifact.artifact_type in high_priority_types.get(agent_type, [])

    def _generate_time_based_instructions(self, time_pressure: str, time_budget: float,
                                        agent_type: str, phase_analysis: Dict[str, Any]) -> List[str]:
        """
        Generate time-based instructions for agents.

        Args:
            time_pressure: Time pressure level
            time_budget: Available time budget
            agent_type: Type of agent
            phase_analysis: Phase time analysis

        Returns:
            List of time-based instructions
        """
        instructions = []

        if time_pressure == "normal":
            instructions.append("Proceed with standard quality and completeness requirements.")
        elif time_pressure == "high":
            instructions.extend([
                " TIME PRESSURE: Focus on critical functionality and core requirements.",
                "Prioritize essential features over nice-to-have enhancements.",
                "Consider simplified approaches where quality impact is minimal."
            ])
        elif time_pressure == "critical":
            instructions.extend([
                " CRITICAL TIME PRESSURE: Focus only on essential deliverables.",
                "Implement minimum viable solution with core functionality.",
                "Defer non-critical features to future iterations."
            ])

        # Add time budget information
        if time_budget:
            instructions.append(f" Available time budget: {time_budget:.1f} hours")

        # Add phase-specific time guidance
        if phase_analysis and phase_analysis.get("time_based_decisions"):
            for decision in phase_analysis["time_based_decisions"]:
                instructions.append(f" Recommended action: {decision}")

        return instructions

    def get_time_based_phase_transition(self, project_id: UUID) -> Dict[str, Any]:
        """
        Determine if phase transition should occur based on time analysis.

        Args:
            project_id: UUID of the project

        Returns:
            Time-based phase transition analysis
        """
        current_phase = self.get_current_phase(project_id)
        time_analysis = self.get_phase_time_analysis(project_id)

        current_phase_analysis = next(
            (p for p in time_analysis["phase_time_analysis"] if p["phase"] == current_phase),
            None
        )

        transition_recommended = False
        transition_reason = ""
        force_transition = False

        if current_phase_analysis:
            # Check if phase has exceeded maximum duration
            if current_phase_analysis["actual_duration_hours"] > current_phase_analysis["max_duration_hours"]:
                transition_recommended = True
                transition_reason = f"Phase exceeded maximum duration ({current_phase_analysis['max_duration_hours']}h)"
                force_transition = True

            # Check if phase is significantly over time pressure threshold
            elif current_phase_analysis["time_status"] == "warning":
                # Check if we can still complete within reasonable time
                remaining_time = current_phase_analysis["max_duration_hours"] - current_phase_analysis["actual_duration_hours"]
                if remaining_time < 2:  # Less than 2 hours remaining
                    transition_recommended = True
                    transition_reason = "Limited time remaining, recommend phase transition"

        return {
            "current_phase": current_phase,
            "transition_recommended": transition_recommended,
            "transition_reason": transition_reason,
            "force_transition": force_transition,
            "time_analysis": current_phase_analysis,
            "next_phase": SDLC_PHASES[current_phase].get("next_phase")
        }

    def get_performance_metrics(self, project_id: UUID) -> Dict[str, Any]:
        """
        Get comprehensive performance metrics for the project.

        Args:
            project_id: UUID of the project

        Returns:
            Performance metrics including time, quality, and efficiency metrics
        """
        # Get time analysis
        time_analysis = self.get_phase_time_analysis(project_id)

        # Get task performance metrics
        project_tasks = self.get_project_tasks(project_id)

        # Calculate task metrics
        total_tasks = len(project_tasks)
        completed_tasks = len([t for t in project_tasks if t.status == TaskStatus.COMPLETED])
        failed_tasks = len([t for t in project_tasks if t.status == TaskStatus.FAILED])
        in_progress_tasks = len([t for t in project_tasks if t.status == TaskStatus.WORKING])

        # Calculate average task duration
        completed_task_durations = []
        for task in project_tasks:
            if task.started_at and task.completed_at:
                duration = task.completed_at - task.started_at
                completed_task_durations.append(duration.total_seconds() / 3600)  # Convert to hours

        avg_task_duration = sum(completed_task_durations) / len(completed_task_durations) if completed_task_durations else 0

        # Calculate agent performance
        agent_performance = {}
        for task in project_tasks:
            agent_type = task.agent_type
            if agent_type not in agent_performance:
                agent_performance[agent_type] = {"total": 0, "completed": 0, "failed": 0}

            agent_performance[agent_type]["total"] += 1
            if task.status == TaskStatus.COMPLETED:
                agent_performance[agent_type]["completed"] += 1
            elif task.status == TaskStatus.FAILED:
                agent_performance[agent_type]["failed"] += 1

        return {
            "project_id": str(project_id),
            "time_metrics": {
                "total_estimated_hours": time_analysis["total_estimated_hours"],
                "total_actual_hours": time_analysis["total_actual_hours"],
                "time_efficiency_percentage": time_analysis["time_efficiency_percentage"],
                "time_variance_percentage": time_analysis["time_variance_percentage"]
            },
            "task_metrics": {
                "total_tasks": total_tasks,
                "completed_tasks": completed_tasks,
                "failed_tasks": failed_tasks,
                "in_progress_tasks": in_progress_tasks,
                "completion_rate": (completed_tasks / total_tasks) * 100 if total_tasks > 0 else 0,
                "average_task_duration_hours": avg_task_duration
            },
            "agent_performance": agent_performance,
            "phase_performance": time_analysis["phase_time_analysis"],
            "recommendations": time_analysis["time_based_recommendations"]
        }

    # ===== CONTEXT GRANULARITY INTEGRATION =====

    def get_integrated_context_summary(self, project_id: UUID, agent_type: str, phase: str,
                                     time_budget_hours: Optional[float] = None,
                                     max_tokens: Optional[int] = None) -> Dict[str, Any]:
        """
        Get comprehensive integrated context summary with all granularity features.

        Args:
            project_id: UUID of the project
            agent_type: Type of agent requesting context
            phase: Current SDLC phase
            time_budget_hours: Available time budget in hours
            max_tokens: Maximum token limit for context

        Returns:
            Complete integrated context summary with all features
        """
        logger.info("Generating integrated context summary",
                   project_id=project_id,
                   agent_type=agent_type,
                   phase=phase)

        # Get time-conscious context
        time_conscious_context = self.get_time_conscious_context(
            project_id, phase, agent_type, time_budget_hours
        )

        # Get context analytics
        context_analytics = self.context_store.get_context_analytics(project_id)

        # Get context recommendations
        context_recommendations = self.context_store.get_context_recommendations(
            project_id, agent_type, phase
        )

        # Get selective context with advanced filtering
        selective_context = self.context_store.get_selective_context(
            project_id=project_id,
            agent_type=agent_type,
            phase=phase,
            max_tokens=max_tokens,
            time_budget_hours=time_budget_hours
        )

        # Get phase progress and time analysis
        phase_progress = self.get_phase_progress(project_id)
        time_analysis = self.get_phase_time_analysis(project_id)

        # Get performance metrics
        performance_metrics = self.get_performance_metrics(project_id)

        # Create comprehensive integration summary
        integrated_summary = {
            "project_id": str(project_id),
            "agent_type": agent_type,
            "phase": phase,
            "timestamp": datetime.now(timezone.utc).isoformat(),

            # Core context data
            "context_ids": time_conscious_context["context_ids"],
            "context_artifacts_count": len(time_conscious_context["context_ids"]),

            # Time-conscious features
            "time_pressure": time_conscious_context["time_pressure"],
            "time_budget_hours": time_budget_hours,
            "time_instructions": time_conscious_context["time_instructions"],
            "context_reduction_percentage": time_conscious_context["context_reduction_percentage"],

            # Selective context features
            "selective_context_stats": selective_context["statistics"],
            "filtering_criteria": selective_context["filtering_criteria"],
            "optimization_info": selective_context["optimization_info"],

            # Analytics and insights
            "context_analytics": context_analytics,
            "context_recommendations": context_recommendations,

            # Project status
            "phase_progress": phase_progress,
            "time_analysis": time_analysis,
            "performance_metrics": performance_metrics,

            # Integration metadata
            "integration_features": {
                "time_conscious_orchestration": True,
                "selective_context_injection": True,
                "context_analytics": True,
                "performance_monitoring": True,
                "recommendation_engine": True
            },

            # Usage optimization
            "context_optimization_score": self._calculate_context_optimization_score(
                time_conscious_context, selective_context, context_analytics
            ),

            # Quality assurance
            "context_quality_indicators": self._assess_context_quality(
                time_conscious_context, selective_context, agent_type, phase
            )
        }

        logger.info("Integrated context summary generated",
                   project_id=project_id,
                   agent_type=agent_type,
                   context_artifacts=integrated_summary["context_artifacts_count"],
                   optimization_score=integrated_summary["context_optimization_score"])

        return integrated_summary

    def _calculate_context_optimization_score(self, time_context: Dict[str, Any],
                                            selective_context: Dict[str, Any],
                                            analytics: Dict[str, Any]) -> float:
        """
        Calculate overall context optimization score.

        Args:
            time_context: Time-conscious context data
            selective_context: Selective context data
            analytics: Context analytics data

        Returns:
            Optimization score from 0-100
        """
        score_components = []

        # Context reduction efficiency (higher reduction = higher score, up to 50%)
        reduction = time_context.get("context_reduction_percentage", 0)
        if reduction > 0:
            # Optimal reduction is 30-50%, penalize over-reduction
            if reduction < 30:
                reduction_score = reduction / 30 * 25  # Max 25 points
            elif reduction <= 50:
                reduction_score = 25  # Full points for optimal range
            else:
                reduction_score = max(0, 25 - (reduction - 50) / 10)  # Penalize over-reduction
            score_components.append(reduction_score)

        # Relevance score from selective context
        relevance_score = selective_context.get("statistics", {}).get("average_relevance_score", 0.5)
        score_components.append(relevance_score * 30)  # Max 30 points

        # Recent activity bonus
        recent_rate = analytics.get("recent_activity_rate", 0)
        score_components.append(recent_rate * 20)  # Max 20 points

        # Diversity bonus (different artifact types)
        artifact_types = len(analytics.get("artifact_types", {}))
        diversity_score = min(artifact_types / 5, 1) * 15  # Max 15 points for 5+ types
        score_components.append(diversity_score)

        # Time pressure adaptation bonus
        time_pressure = time_context.get("time_pressure", "normal")
        adaptation_bonus = {"normal": 0, "high": 5, "critical": 10}.get(time_pressure, 0)
        score_components.append(adaptation_bonus)

        total_score = sum(score_components)

        # Normalize to 0-100 scale
        return min(100.0, max(0.0, total_score))

    def _assess_context_quality(self, time_context: Dict[str, Any],
                              selective_context: Dict[str, Any],
                              agent_type: str, phase: str) -> Dict[str, Any]:
        """
        Assess the quality of the provided context.

        Args:
            time_context: Time-conscious context data
            selective_context: Selective context data
            agent_type: Type of agent
            phase: Current phase

        Returns:
            Context quality assessment
        """
        quality_indicators = {
            "relevance_score": selective_context.get("statistics", {}).get("average_relevance_score", 0),
            "context_completeness": self._assess_context_completeness(time_context, agent_type, phase),
            "time_adaptation_quality": self._assess_time_adaptation_quality(time_context),
            "diversity_score": self._calculate_diversity_score(selective_context),
            "optimization_effectiveness": self._assess_optimization_effectiveness(time_context, selective_context)
        }

        # Calculate overall quality score
        quality_weights = {
            "relevance_score": 0.3,
            "context_completeness": 0.25,
            "time_adaptation_quality": 0.2,
            "diversity_score": 0.15,
            "optimization_effectiveness": 0.1
        }

        overall_quality = sum(
            quality_indicators[key] * weight
            for key, weight in quality_weights.items()
        )

        quality_indicators["overall_quality_score"] = overall_quality
        quality_indicators["quality_rating"] = self._get_quality_rating(overall_quality)

        return quality_indicators

    def _assess_context_completeness(self, time_context: Dict[str, Any],
                                   agent_type: str, phase: str) -> float:
        """Assess how complete the context is for the agent and phase."""
        context_ids = time_context.get("context_ids", [])
        context_count = len(context_ids)

        # Define expected context ranges by agent and phase
        expected_ranges = {
            "analyst": {"discovery": (3, 8), "plan": (2, 6), "design": (1, 4), "build": (1, 3), "validate": (1, 3), "launch": (1, 2)},
            "architect": {"discovery": (1, 3), "plan": (4, 10), "design": (3, 8), "build": (2, 5), "validate": (1, 3), "launch": (1, 3)},
            "coder": {"discovery": (1, 2), "plan": (2, 5), "design": (3, 8), "build": (5, 15), "validate": (2, 6), "launch": (1, 3)},
            "tester": {"discovery": (1, 2), "plan": (1, 3), "design": (2, 5), "build": (3, 8), "validate": (5, 12), "launch": (1, 3)},
            "deployer": {"discovery": (0, 1), "plan": (1, 2), "design": (1, 3), "build": (2, 5), "validate": (1, 3), "launch": (3, 8)}
        }

        expected_min, expected_max = expected_ranges.get(agent_type, {}).get(phase, (1, 5))

        if context_count < expected_min:
            completeness = context_count / expected_min * 0.7  # Penalty for too few
        elif context_count <= expected_max:
            completeness = 1.0  # Optimal range
        else:
            completeness = max(0.8, 1.0 - (context_count - expected_max) / expected_max * 0.2)  # Minor penalty for excess

        return completeness

    def _assess_time_adaptation_quality(self, time_context: Dict[str, Any]) -> float:
        """Assess how well the context adapts to time constraints."""
        time_pressure = time_context.get("time_pressure", "normal")
        reduction_percentage = time_context.get("context_reduction_percentage", 0)

        if time_pressure == "normal":
            # Should have minimal reduction
            return max(0.8, 1.0 - reduction_percentage / 20)
        elif time_pressure == "high":
            # Should have moderate reduction (20-40%)
            if 20 <= reduction_percentage <= 40:
                return 1.0
            elif reduction_percentage < 20:
                return 0.7  # Too little reduction
            else:
                return max(0.6, 1.0 - (reduction_percentage - 40) / 20)
        elif time_pressure == "critical":
            # Should have significant reduction (40-60%)
            if 40 <= reduction_percentage <= 60:
                return 1.0
            elif reduction_percentage < 40:
                return 0.6  # Too little reduction
            else:
                return max(0.5, 1.0 - (reduction_percentage - 60) / 20)

        return 0.8  # Default good score

    def _calculate_diversity_score(self, selective_context: Dict[str, Any]) -> float:
        """Calculate context diversity score based on artifact types."""
        selected_artifacts = selective_context.get("selected_artifacts", [])

        if not selected_artifacts:
            return 0.0

        # Count unique artifact types
        artifact_types = set()
        for artifact_data in selected_artifacts:
            artifact = artifact_data.get("artifact")
            if artifact and hasattr(artifact, 'artifact_type'):
                artifact_types.add(artifact.artifact_type)

        unique_types = len(artifact_types)
        total_artifacts = len(selected_artifacts)

        # Diversity score based on type variety relative to total artifacts
        diversity_ratio = unique_types / max(total_artifacts, 1)

        # Bonus for having multiple types
        if unique_types >= 3:
            diversity_ratio *= 1.2
        if unique_types >= 5:
            diversity_ratio *= 1.1

        return min(1.0, diversity_ratio)

    def _assess_optimization_effectiveness(self, time_context: Dict[str, Any],
                                         selective_context: Dict[str, Any]) -> float:
        """Assess how effective the context optimization is."""
        # Check if optimization was applied and its effectiveness
        optimization_info = selective_context.get("optimization_info", {})
        if not optimization_info.get("optimization_applied", False):
            return 0.8  # No optimization needed

        original_tokens = optimization_info.get("original_total_tokens", 0)
        final_tokens = optimization_info.get("final_total_tokens", 0)
        max_tokens = optimization_info.get("max_tokens", 0)

        if original_tokens == 0:
            return 0.5  # Cannot assess

        # Calculate how well we stayed within limits
        if final_tokens <= max_tokens:
            if final_tokens <= max_tokens * 0.9:  # Well within limits
                return 1.0
            else:  # Close to limit
                return 0.9
        else:
            # Exceeded limit - penalize based on overrun
            overrun_ratio = (final_tokens - max_tokens) / max_tokens
            return max(0.3, 1.0 - overrun_ratio)

    def _get_quality_rating(self, quality_score: float) -> str:
        """Convert quality score to rating."""
        if quality_score >= 0.9:
            return "EXCELLENT"
        elif quality_score >= 0.8:
            return "VERY_GOOD"
        elif quality_score >= 0.7:
            return "GOOD"
        elif quality_score >= 0.6:
            return "FAIR"
        elif quality_score >= 0.5:
            return "POOR"
        else:
            return "CRITICAL"

    def get_context_granularity_report(self, project_id: UUID) -> Dict[str, Any]:
        """
        Generate comprehensive context granularity report for the project.

        Args:
            project_id: UUID of the project

        Returns:
            Complete context granularity report
        """
        logger.info("Generating context granularity report", project_id=project_id)

        # Get all project artifacts
        all_artifacts = self.context_store.get_artifacts_by_project(project_id)

        # Get context analytics
        analytics = self.context_store.get_context_analytics(project_id)

        # Analyze artifact distribution
        artifact_distribution = self._analyze_artifact_distribution(all_artifacts)

        # Analyze context usage patterns
        usage_patterns = self._analyze_context_usage_patterns(project_id)

        # Generate optimization recommendations
        optimization_recommendations = self._generate_context_optimization_recommendations(
            analytics, artifact_distribution, usage_patterns
        )

        report = {
            "project_id": str(project_id),
            "report_generated_at": datetime.now(timezone.utc).isoformat(),
            "total_artifacts": len(all_artifacts),

            # Analytics summary
            "analytics_summary": analytics,

            # Distribution analysis
            "artifact_distribution": artifact_distribution,

            # Usage patterns
            "usage_patterns": usage_patterns,

            # Optimization recommendations
            "optimization_recommendations": optimization_recommendations,

            # Granularity metrics
            "granularity_metrics": {
                "average_artifacts_per_type": len(all_artifacts) / max(len(analytics.get("artifact_types", {})), 1),
                "recent_activity_ratio": analytics.get("recent_activity_rate", 0),
                "context_density_score": self._calculate_context_density_score(all_artifacts),
                "temporal_distribution_score": self._calculate_temporal_distribution_score(analytics)
            },

            # Feature utilization
            "feature_utilization": {
                "selective_context_enabled": True,
                "time_conscious_filtering_enabled": True,
                "context_analytics_enabled": True,
                "recommendation_engine_enabled": True,
                "optimization_engine_enabled": True
            }
        }

        logger.info("Context granularity report generated",
                   project_id=project_id,
                   total_artifacts=len(all_artifacts),
                   recommendations_count=len(optimization_recommendations))

        return report

    def _analyze_artifact_distribution(self, artifacts: List) -> Dict[str, Any]:
        """Analyze the distribution of artifacts by type, agent, and time."""
        distribution = {
            "by_type": {},
            "by_agent": {},
            "by_time_period": {
                "last_24h": 0,
                "last_7d": 0,
                "last_30d": 0,
                "older": 0
            },
            "size_distribution": {
                "small": 0,  # < 500 tokens
                "medium": 0,  # 500-2000 tokens
                "large": 0,   # 2000-5000 tokens
                "xlarge": 0   # > 5000 tokens
            }
        }

        now = datetime.now(timezone.utc)

        for artifact in artifacts:
            # By type
            art_type = getattr(artifact, 'artifact_type', 'unknown')
            distribution["by_type"][art_type] = distribution["by_type"].get(art_type, 0) + 1

            # By agent
            agent = getattr(artifact, 'source_agent', 'unknown')
            distribution["by_agent"][agent] = distribution["by_agent"].get(agent, 0) + 1

            # By time period
            if hasattr(artifact, 'created_at') and artifact.created_at:
                time_diff = now - artifact.created_at
                hours_diff = time_diff.total_seconds() / 3600

                if hours_diff <= 24:
                    distribution["by_time_period"]["last_24h"] += 1
                elif hours_diff <= 168:  # 7 days
                    distribution["by_time_period"]["last_7d"] += 1
                elif hours_diff <= 720:  # 30 days
                    distribution["by_time_period"]["last_30d"] += 1
                else:
                    distribution["by_time_period"]["older"] += 1

            # By size
            estimated_tokens = self._estimate_artifact_tokens(artifact)
            if estimated_tokens < 500:
                distribution["size_distribution"]["small"] += 1
            elif estimated_tokens < 2000:
                distribution["size_distribution"]["medium"] += 1
            elif estimated_tokens < 5000:
                distribution["size_distribution"]["large"] += 1
            else:
                distribution["size_distribution"]["xlarge"] += 1

        return distribution

    def _analyze_context_usage_patterns(self, project_id: UUID) -> Dict[str, Any]:
        """Analyze patterns in context usage across the project."""
        # Get project tasks to analyze context usage
        project_tasks = self.get_project_tasks(project_id)

        usage_patterns = {
            "average_context_per_task": 0,
            "context_reuse_patterns": {},
            "agent_context_preferences": {},
            "temporal_context_usage": {
                "morning": 0,  # 6-12
                "afternoon": 0,  # 12-18
                "evening": 0,    # 18-24
                "night": 0       # 0-6
            }
        }

        total_context_items = 0
        context_usage_by_hour = {}

        for task in project_tasks:
            context_count = len(getattr(task, 'context_ids', []))
            total_context_items += context_count

            # Track agent preferences
            agent = getattr(task, 'agent_type', 'unknown')
            if agent not in usage_patterns["agent_context_preferences"]:
                usage_patterns["agent_context_preferences"][agent] = []
            usage_patterns["agent_context_preferences"][agent].append(context_count)

            # Track temporal patterns
            if hasattr(task, 'created_at') and task.created_at:
                hour = task.created_at.hour
                if 6 <= hour < 12:
                    usage_patterns["temporal_context_usage"]["morning"] += context_count
                elif 12 <= hour < 18:
                    usage_patterns["temporal_context_usage"]["afternoon"] += context_count
                elif 18 <= hour < 24:
                    usage_patterns["temporal_context_usage"]["evening"] += context_count
                else:
                    usage_patterns["temporal_context_usage"]["night"] += context_count

        # Calculate averages
        if project_tasks:
            usage_patterns["average_context_per_task"] = total_context_items / len(project_tasks)

            # Calculate average context per agent
            for agent, context_counts in usage_patterns["agent_context_preferences"].items():
                usage_patterns["agent_context_preferences"][agent] = sum(context_counts) / len(context_counts)

        return usage_patterns

    def _generate_context_optimization_recommendations(self, analytics: Dict[str, Any],
                                                     distribution: Dict[str, Any],
                                                     usage_patterns: Dict[str, Any]) -> List[str]:
        """Generate context optimization recommendations."""
        recommendations = []

        # Analyze artifact distribution
        type_distribution = distribution.get("by_type", {})
        if len(type_distribution) < 3:
            recommendations.append("Consider diversifying artifact types for better context coverage")

        # Analyze size distribution
        size_dist = distribution.get("size_distribution", {})
        large_artifacts = size_dist.get("large", 0) + size_dist.get("xlarge", 0)
        if large_artifacts > len(type_distribution) * 0.3:
            recommendations.append("Consider breaking down large artifacts into smaller, focused pieces")

        # Analyze temporal distribution
        time_dist = distribution.get("by_time_period", {})
        recent_ratio = (time_dist.get("last_24h", 0) + time_dist.get("last_7d", 0)) / max(sum(time_dist.values()), 1)
        if recent_ratio < 0.4:
            recommendations.append("Increase recent artifact generation for better context relevance")

        # Analyze usage patterns
        avg_context = usage_patterns.get("average_context_per_task", 0)
        if avg_context > 10:
            recommendations.append("Consider optimizing context size - average context per task is high")
        elif avg_context < 3:
            recommendations.append("Consider enriching context - average context per task is low")

        # Agent-specific recommendations
        agent_prefs = usage_patterns.get("agent_context_preferences", {})
        for agent, avg_context in agent_prefs.items():
            if avg_context > 8:
                recommendations.append(f"Optimize context for {agent} agent - using more context than average")
            elif avg_context < 2:
                recommendations.append(f"Enrich context for {agent} agent - using less context than optimal")

        return recommendations

    def _calculate_context_density_score(self, artifacts: List) -> float:
        """Calculate context density score."""
        if not artifacts:
            return 0.0

        # Measure how densely information is packed
        total_tokens = sum(self._estimate_artifact_tokens(art) for art in artifacts)
        avg_tokens_per_artifact = total_tokens / len(artifacts)

        # Optimal density is around 1000-2000 tokens per artifact
        if 1000 <= avg_tokens_per_artifact <= 2000:
            return 1.0
        elif 500 <= avg_tokens_per_artifact < 1000:
            return 0.8
        elif 2000 < avg_tokens_per_artifact <= 3000:
            return 0.7
        else:
            return 0.5

    def _calculate_temporal_distribution_score(self, analytics: Dict[str, Any]) -> float:
        """Calculate temporal distribution score."""
        time_dist = analytics.get("temporal_distribution", {})
        total_artifacts = analytics.get("total_artifacts", 0)

        if total_artifacts == 0:
            return 0.0

        # Prefer recent artifacts (last 24h and 7d)
        recent_artifacts = time_dist.get("last_24h", 0) + time_dist.get("last_7d", 0)
        recent_ratio = recent_artifacts / total_artifacts

        # Score based on recency
        if recent_ratio >= 0.6:
            return 1.0
        elif recent_ratio >= 0.4:
            return 0.8
        elif recent_ratio >= 0.2:
            return 0.6
        else:
            return 0.4

    def _estimate_artifact_tokens(self, artifact) -> int:
        """Estimate token count for an artifact."""
        # Rough estimation: 1 token per 4 characters for English text
        content_str = ""
        if hasattr(artifact, 'content'):
            content_str = str(artifact.content)
        return len(content_str) // 4
