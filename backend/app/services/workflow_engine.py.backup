"""
Workflow Execution Engine for BMAD Core Template System

This module implements the workflow execution engine that manages dynamic workflow
execution, agent handoffs, state persistence, and recovery mechanisms.
"""

import asyncio
from typing import Any, Dict, List, Optional, Union, Callable
from datetime import datetime, timezone
from uuid import UUID, uuid4
import structlog
import json

from sqlalchemy.orm import Session
from sqlalchemy import and_

from app.models.workflow import WorkflowDefinition, WorkflowStep, WorkflowExecutionState
from app.models.workflow_state import (
    WorkflowExecutionStateModel,
    WorkflowStepExecutionState,
    WorkflowExecutionState as ExecutionStateEnum,
    WorkflowRecoveryPoint
)
from app.models.handoff import HandoffSchema
from app.models.task import Task, TaskStatus
from app.models.agent import AgentType
from app.database.models import WorkflowStateDB, TaskDB, ProjectDB
from app.services.workflow_service import WorkflowService
from app.services.context_store import ContextStoreService
from app.services.autogen_service import AutoGenService
from app.services.hitl_service import HitlService
from app.services.workflow_execution_manager import WorkflowExecutionManager
from app.services.workflow_step_processor import WorkflowStepProcessor
from app.services.workflow_persistence_manager import WorkflowPersistenceManager
from app.services.workflow_hitl_integrator import WorkflowHitlIntegrator
# Lazy import to avoid circular dependency
from app.websocket.manager import websocket_manager
from app.websocket.events import WebSocketEvent, EventType

logger = structlog.get_logger(__name__)


class WorkflowExecutionEngine:
    """
    Workflow execution engine with state machine pattern.

    This engine manages the complete lifecycle of workflow execution including:
    - Dynamic workflow loading and execution
    - Agent handoff coordination
    - State persistence and recovery
    - Conditional workflow routing
    - Parallel task execution
    - Progress tracking and milestones
    """

    def __init__(self, db: Session):
        self.db = db
        self.workflow_service = WorkflowService()
        self.context_store = ContextStoreService(db)
        self.autogen_service = AutoGenService()
        self.hitl_service = HitlService(db)

        # Execution state cache
        self._active_executions: Dict[str, WorkflowExecutionStateModel] = {}

        # Recovery mechanisms
        self._recovery_handlers: Dict[str, Callable] = {}

    async def start_workflow_execution(
        self,
        workflow_id: str,
        project_id: str,
        context_data: Optional[Dict[str, Any]] = None
    ) -> WorkflowExecutionStateModel:
        """
        Start execution of a workflow.

        Args:
            workflow_id: ID of the workflow to execute
            project_id: ID of the project
            context_data: Initial context data

        Returns:
            WorkflowExecutionStateModel representing the started execution

        Raises:
            ValueError: If workflow cannot be started
        """
        try:
            logger.info("Starting workflow execution", workflow_id=workflow_id, project_id=project_id)

            # Load workflow definition
            workflow = self.workflow_service.load_workflow(workflow_id)

            # Create execution state
            execution = WorkflowExecutionStateModel(
                project_id=project_id,
                workflow_id=workflow_id,
                total_steps=len(workflow.sequence),
                context_data=context_data or {},
                status=ExecutionStateEnum.PENDING
            )

            # Initialize steps
            for i, step in enumerate(workflow.sequence):
                execution_step = WorkflowStepExecutionState(
                    step_index=i,
                    agent=step.agent
                )
                execution.steps.append(execution_step)

            # Persist execution state
            self._persist_execution_state(execution)

            # Cache active execution
            self._active_executions[execution.execution_id] = execution

            # Mark as started
            execution.mark_started()
            self._persist_execution_state(execution)

            # Emit WebSocket event
            await self._emit_workflow_event(
                EventType.WORKFLOW_EVENT,
                project_id,
                execution.execution_id,
                {"event": "started", "workflow_id": workflow_id, "total_steps": len(workflow.sequence)}
            )

            logger.info("Workflow execution started",
                       execution_id=execution.execution_id,
                       workflow_id=workflow_id,
                       total_steps=len(workflow.sequence))

            return execution

        except Exception as e:
            logger.error("Failed to start workflow execution",
                        workflow_id=workflow_id,
                        project_id=project_id,
                        error=str(e))
            raise ValueError(f"Failed to start workflow execution: {str(e)}")

    async def execute_workflow_step(
        self,
        execution_id: str,
        step_index: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Execute a specific workflow step or the next pending step.

        Args:
            execution_id: ID of the workflow execution
            step_index: Specific step to execute, or None for next pending step

        Returns:
            Dictionary with execution results

        Raises:
            ValueError: If step execution fails
        """
        execution = self._get_execution_state(execution_id)
        if not execution:
            raise ValueError(f"Execution {execution_id} not found")

        if execution.is_complete():
            return {"status": "completed", "message": "Workflow execution is already complete"}

        # Determine which step to execute
        if step_index is not None:
            if step_index >= len(execution.steps):
                raise ValueError(f"Step index {step_index} is out of range")
            step = execution.steps[step_index]
        else:
            step = execution.get_next_pending_step()

        if not step:
            return {"status": "no_pending_steps", "message": "No pending steps found"}

        try:
            logger.info("Executing workflow step",
                       execution_id=execution_id,
                       step_index=step.step_index,
                       agent=step.agent)

            # Mark step as running
            step.status = ExecutionStateEnum.RUNNING
            step.started_at = datetime.now(timezone.utc).isoformat()
            self._persist_execution_state(execution)

            # Load workflow definition
            workflow = self.workflow_service.load_workflow(execution.workflow_id)
            workflow_step = workflow.get_step_by_index(step.step_index)

            if not workflow_step:
                raise ValueError(f"Workflow step {step.step_index} not found in workflow definition")

            # Check conditional execution
            if workflow_step.condition and not self._evaluate_condition(workflow_step.condition, execution.context_data):
                logger.info("Skipping conditional step",
                           execution_id=execution_id,
                           step_index=step.step_index,
                           condition=workflow_step.condition)
                step.status = ExecutionStateEnum.COMPLETED
                step.completed_at = datetime.now(timezone.utc).isoformat()
                self._persist_execution_state(execution)
                return {"status": "skipped", "message": "Step condition not met"}

            # Create task for agent
            task = await self._create_agent_task(
                execution.project_id,
                step.agent,
                workflow_step,
                execution.context_data
            )

            # Execute task with AutoGen
            result = await self._execute_agent_task(task, execution)

            # Update step with results
            step.status = ExecutionStateEnum.COMPLETED
            step.completed_at = datetime.now(timezone.utc).isoformat()
            step.result = result
            step.task_id = str(task.task_id)

            # Add created artifacts
            if result.get("artifacts"):
                for artifact_id in result["artifacts"]:
                    step.artifacts_created.append(artifact_id)
                    execution.add_artifact(artifact_id)

            # Update context data with step results
            self._update_context_from_step_result(execution, workflow_step, result)

            # Persist updated state
            self._persist_execution_state(execution)

            # Emit WebSocket event
            await self._emit_workflow_event(
                EventType.WORKFLOW_EVENT,
                execution.project_id,
                execution_id,
                {
                    "event": "step_completed",
                    "step_index": step.step_index,
                    "agent": step.agent,
                    "artifacts_created": step.artifacts_created
                }
            )

            # Check for HITL triggers after step completion
            hitl_request = await self._check_hitl_triggers_after_step(
                execution, step, result
            )

            if hitl_request:
                # Pause workflow for HITL
                await self.pause_workflow_execution(
                    execution_id,
                    f"HITL approval required for step {step.step_index}"
                )

                logger.info("Workflow paused for HITL approval",
                           execution_id=execution_id,
                           hitl_request_id=str(hitl_request.request_id),
                           step_index=step.step_index)

                return {
                    "status": "paused_for_hitl",
                    "step_index": step.step_index,
                    "agent": step.agent,
                    "result": result,
                    "hitl_request_id": str(hitl_request.request_id)
                }

            # Check if workflow is complete
            if execution.get_next_pending_step() is None:
                execution.mark_completed()
                self._persist_execution_state(execution)

                await self._emit_workflow_event(
                    EventType.WORKFLOW_EVENT,
                    execution.project_id,
                    execution_id,
                    {"event": "completed", "total_steps": len(execution.steps)}
                )

            logger.info("Workflow step completed",
                       execution_id=execution_id,
                       step_index=step.step_index,
                       agent=step.agent)

            return {
                "status": "completed",
                "step_index": step.step_index,
                "agent": step.agent,
                "result": result
            }

        except Exception as e:
            logger.error("Workflow step execution failed",
                        execution_id=execution_id,
                        step_index=step.step_index,
                        agent=step.agent,
                        error=str(e))

            # Mark step as failed
            step.status = ExecutionStateEnum.FAILED
            step.error_message = str(e)
            step.completed_at = datetime.now(timezone.utc).isoformat()
            execution.mark_failed(f"Step {step.step_index} failed: {str(e)}")
            self._persist_execution_state(execution)

            # Emit failure event
            await self._emit_workflow_event(
                EventType.ERROR,
                execution.project_id,
                execution_id,
                {
                    "event": "step_failed",
                    "step_index": step.step_index,
                    "agent": step.agent,
                    "error": str(e)
                }
            )

            raise ValueError(f"Step execution failed: {str(e)}")

    async def execute_parallel_steps(
        self,
        execution_id: str,
        step_indices: List[int]
    ) -> Dict[str, Any]:
        """
        Execute multiple workflow steps in parallel.

        Args:
            execution_id: ID of the workflow execution
            step_indices: List of step indices to execute in parallel

        Returns:
            Dictionary with parallel execution results
        """
        execution = self._get_execution_state(execution_id)
        if not execution:
            raise ValueError(f"Execution {execution_id} not found")

        logger.info("Executing parallel workflow steps",
                   execution_id=execution_id,
                   step_indices=step_indices)

        # Create tasks for parallel execution
        tasks = []
        for step_index in step_indices:
            task = asyncio.create_task(self.execute_workflow_step(execution_id, step_index))
            tasks.append(task)

        # Wait for all tasks to complete
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # Process results
        success_count = 0
        failure_count = 0
        step_results = {}

        for i, result in enumerate(results):
            step_index = step_indices[i]
            if isinstance(result, Exception):
                failure_count += 1
                step_results[step_index] = {"status": "failed", "error": str(result)}
            else:
                success_count += 1
                step_results[step_index] = result

        logger.info("Parallel workflow steps completed",
                   execution_id=execution_id,
                   success_count=success_count,
                   failure_count=failure_count)

        return {
            "status": "completed" if failure_count == 0 else "partial_failure",
            "success_count": success_count,
            "failure_count": failure_count,
            "step_results": step_results
        }

    async def pause_workflow_execution(self, execution_id: str, reason: str) -> bool:
        """
        Pause a workflow execution.

        Args:
            execution_id: ID of the workflow execution
            reason: Reason for pausing

        Returns:
            True if paused successfully
        """
        execution = self._get_execution_state(execution_id)
        if not execution:
            return False

        execution.pause(reason)
        self._persist_execution_state(execution)

        await self._emit_workflow_event(
            EventType.WORKFLOW_EVENT,
            execution.project_id,
            execution_id,
            {"event": "paused", "reason": reason}
        )

        logger.info("Workflow execution paused", execution_id=execution_id, reason=reason)
        return True

    async def resume_workflow_execution(self, execution_id: str) -> bool:
        """
        Resume a paused workflow execution.

        Args:
            execution_id: ID of the workflow execution

        Returns:
            True if resumed successfully
        """
        execution = self._get_execution_state(execution_id)
        if not execution or not execution.can_resume():
            return False

        execution.resume()
        self._persist_execution_state(execution)

        await self._emit_workflow_event(
            EventType.WORKFLOW_EVENT,
            execution.project_id,
            execution_id,
            {"event": "resumed"}
        )

        logger.info("Workflow execution resumed", execution_id=execution_id)
        return True

    async def cancel_workflow_execution(self, execution_id: str, reason: str) -> bool:
        """
        Cancel a workflow execution.

        Args:
            execution_id: ID of the workflow execution
            reason: Reason for cancellation

        Returns:
            True if cancelled successfully
        """
        execution = self._get_execution_state(execution_id)
        if not execution:
            return False

        execution.cancel(reason)
        self._persist_execution_state(execution)

        await self._emit_workflow_event(
            EventType.WORKFLOW_EVENT,
            execution.project_id,
            execution_id,
            {"event": "cancelled", "reason": reason}
        )

        logger.info("Workflow execution cancelled", execution_id=execution_id, reason=reason)
        return True

    def recover_workflow_execution(self, execution_id: str) -> Optional[WorkflowExecutionStateModel]:
        """
        Recover a workflow execution from persisted state.

        Args:
            execution_id: ID of the workflow execution

        Returns:
            Recovered WorkflowExecutionStateModel or None if not found
        """
        try:
            # Load from database
            db_state = self.db.query(WorkflowStateDB).filter(
                WorkflowStateDB.execution_id == execution_id
            ).first()

            if not db_state:
                return None

            # Convert database model to Pydantic model
            execution = WorkflowExecutionStateModel(
                execution_id=db_state.execution_id,
                project_id=str(db_state.project_id),
                workflow_id=db_state.workflow_id,
                status=ExecutionStateEnum(db_state.status),
                current_step=db_state.current_step,
                total_steps=db_state.total_steps,
                steps=db_state.steps_data or [],
                context_data=db_state.context_data or {},
                created_artifacts=db_state.created_artifacts or [],
                error_message=db_state.error_message,
                started_at=db_state.started_at.isoformat() if db_state.started_at else None,
                completed_at=db_state.completed_at.isoformat() if db_state.completed_at else None,
                created_at=db_state.created_at.isoformat() if isinstance(db_state.created_at, datetime) else db_state.created_at,
                updated_at=db_state.updated_at.isoformat() if isinstance(db_state.updated_at, datetime) else db_state.updated_at
            )

            # Cache recovered execution
            self._active_executions[execution_id] = execution

            logger.info("Workflow execution recovered", execution_id=execution_id)
            return execution

        except Exception as e:
            logger.error("Failed to recover workflow execution",
                        execution_id=execution_id,
                        error=str(e))
            return None

    def get_workflow_execution_status(self, execution_id: str) -> Optional[Dict[str, Any]]:
        """
        Get the current status of a workflow execution.

        Args:
            execution_id: ID of the workflow execution

        Returns:
            Dictionary with execution status information
        """
        execution = self._get_execution_state(execution_id)
        if not execution:
            return None

        workflow = self.workflow_service.load_workflow(execution.workflow_id)

        return {
            "execution_id": execution.execution_id,
            "workflow_id": execution.workflow_id,
            "project_id": execution.project_id,
            "status": execution.status.value,
            "current_step": execution.current_step,
            "total_steps": execution.total_steps,
            "completed_steps": len(execution.get_completed_steps()),
            "pending_steps": len(execution.get_pending_steps()),
            "failed_steps": len(execution.get_failed_steps()),
            "started_at": execution.started_at,
            "completed_at": execution.completed_at,
            "error_message": execution.error_message,
            "workflow_name": workflow.name,
            "workflow_description": workflow.description,
            "can_resume": execution.can_resume(),
            "is_complete": execution.is_complete()
        }

    def resume_workflow_execution_sync(self, execution_id: str) -> bool:
        """
        Synchronous wrapper for resuming workflow execution.

        Args:
            execution_id: ID of the workflow execution

        Returns:
            True if resumed successfully
        """
        import asyncio

        async def _resume():
            return await self.resume_workflow_execution(execution_id)

        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If loop is already running, we need to handle differently
                try:
                    import nest_asyncio
                    nest_asyncio.apply()
                    return loop.run_until_complete(_resume())
                except ImportError:
                    logger.warning("nest_asyncio not available, cannot resume workflow in running event loop")
                    return False
            else:
                return loop.run_until_complete(_resume())
        except RuntimeError:
            # No event loop, create new one
            return asyncio.run(_resume())

    async def generate_agent_handoff(
        self,
        execution_id: str,
        from_agent: str,
        to_agent: str
    ) -> Optional[HandoffSchema]:
        """
        Generate a structured handoff between agents.

        Args:
            execution_id: ID of the workflow execution
            from_agent: Agent handing off
            to_agent: Agent receiving handoff

        Returns:
            HandoffSchema for agent transition
        """
        execution = self._get_execution_state(execution_id)
        if not execution:
            return None

        # Generate handoff using workflow service
        handoff = self.workflow_service.generate_handoff(
            execution_id,
            from_agent,
            to_agent,
            execution.context_data
        )

        if handoff:
            logger.info("Generated agent handoff",
                       execution_id=execution_id,
                       from_agent=from_agent,
                       to_agent=to_agent,
                       handoff_id=handoff.handoff_id)

        return handoff

    def _get_execution_state(self, execution_id: str) -> Optional[WorkflowExecutionStateModel]:
        """Get execution state from cache or recover from database."""
        # Check cache first
        if execution_id in self._active_executions:
            return self._active_executions[execution_id]

        # Try to recover from database
        return self.recover_workflow_execution(execution_id)

    def _persist_execution_state(self, execution: WorkflowExecutionStateModel) -> None:
        """Persist execution state to database."""
        try:
            # Convert to database format
            steps_data = [step.model_dump() for step in execution.steps]

            # Convert UUID objects to strings in context_data and created_artifacts for JSON serialization
            context_data = self._convert_uuids_to_strings(execution.context_data)
            created_artifacts = [str(artifact_id) if isinstance(artifact_id, UUID) else artifact_id
                               for artifact_id in execution.created_artifacts]

            # Find existing record or create new
            db_state = self.db.query(WorkflowStateDB).filter(
                WorkflowStateDB.execution_id == execution.execution_id
            ).first()

            if db_state:
                # Update existing
                db_state.status = execution.status.value
                db_state.current_step = execution.current_step
                db_state.total_steps = execution.total_steps
                db_state.steps_data = steps_data
                db_state.context_data = context_data
                db_state.created_artifacts = created_artifacts
                db_state.error_message = execution.error_message
                db_state.started_at = datetime.fromisoformat(execution.started_at) if execution.started_at else None
                db_state.completed_at = datetime.fromisoformat(execution.completed_at) if execution.completed_at else None
                db_state.updated_at = datetime.now(timezone.utc)
            else:
                # Create new
                db_state = WorkflowStateDB(
                    project_id=UUID(execution.project_id),
                    workflow_id=execution.workflow_id,
                    execution_id=execution.execution_id,
                    status=execution.status.value,
                    current_step=execution.current_step,
                    total_steps=execution.total_steps,
                    steps_data=steps_data,
                    context_data=context_data,
                    created_artifacts=created_artifacts,
                    error_message=execution.error_message,
                    started_at=datetime.fromisoformat(execution.started_at) if execution.started_at else None,
                    completed_at=datetime.fromisoformat(execution.completed_at) if execution.completed_at else None
                )
                self.db.add(db_state)

            self.db.commit()

        except Exception as e:
            logger.error("Failed to persist execution state",
                        execution_id=execution.execution_id,
                        error=str(e))
            self.db.rollback()
            raise

    def _evaluate_condition(self, condition: str, context_data: Dict[str, Any]) -> bool:
        """Evaluate a conditional expression against context data."""
        try:
            # Simple condition evaluation - can be extended for complex expressions
            if condition.startswith("context."):
                key = condition[8:]  # Remove "context." prefix
                return bool(context_data.get(key))

            # Support for more complex conditions
            if condition == "always_true":
                return True
            elif condition == "always_false":
                return False
            elif condition.startswith("not_empty:"):
                key = condition[9:]  # Remove "not_empty:" prefix
                value = context_data.get(key)
                return value is not None and str(value).strip() != ""
            elif condition.startswith("equals:"):
                # Format: equals:key,value
                parts = condition[7:].split(",", 1)
                if len(parts) == 2:
                    key, expected_value = parts
                    actual_value = context_data.get(key)
                    return str(actual_value) == expected_value

            # Default to True for unrecognized conditions
            return True

        except Exception as e:
            logger.warning("Failed to evaluate condition",
                          condition=condition,
                          error=str(e))
            return False

    async def _create_agent_task(
        self,
        project_id: str,
        agent_type: str,
        workflow_step: WorkflowStep,
        context_data: Dict[str, Any]
    ) -> Task:
        """Create a task for agent execution."""
        # Generate task instructions from workflow step
        instructions = workflow_step.action or f"Execute {workflow_step.agent} task"

        # Create task record
        task_data = {
            "project_id": UUID(project_id),
            "agent_type": agent_type,
            "instructions": instructions,
            "context_ids": []  # Will be populated with relevant artifacts
        }

        db_task = TaskDB(**task_data)
        self.db.add(db_task)
        self.db.commit()
        self.db.refresh(db_task)

        # Convert to Pydantic model
        task = Task(
            task_id=db_task.id,
            project_id=db_task.project_id,
            agent_type=db_task.agent_type,
            status=db_task.status,
            context_ids=[UUID(cid) for cid in db_task.context_ids],
            instructions=db_task.instructions,
            output=db_task.output,
            error_message=db_task.error_message,
            created_at=db_task.created_at,
            updated_at=db_task.updated_at,
            started_at=db_task.started_at,
            completed_at=db_task.completed_at
        )

        return task

    async def _execute_agent_task(
        self,
        task: Task,
        execution: WorkflowExecutionStateModel
    ) -> Dict[str, Any]:
        """Execute a task using AutoGen service."""
        # Get context artifacts
        context_artifacts = []
        if task.context_ids:
            context_artifacts = self.context_store.get_artifacts_by_ids(task.context_ids)

        # Create handoff schema for agent coordination
        handoff = HandoffSchema(
            from_agent="orchestrator",
            to_agent=task.agent_type,
            phase=f"workflow_{execution.workflow_id}",
            context_ids=task.context_ids,
            instructions=task.instructions,
            expected_outputs=["task_result"],
            metadata={
                "execution_id": execution.execution_id,
                "workflow_id": execution.workflow_id,
                "step_index": execution.current_step
            }
        )

        # Execute with AutoGen
        result = await self.autogen_service.execute_task(task, handoff, context_artifacts)

        return result

    def _update_context_from_step_result(
        self,
        execution: WorkflowExecutionStateModel,
        workflow_step: WorkflowStep,
        result: Dict[str, Any]
    ) -> None:
        """Update execution context with step results."""
        if workflow_step.creates:
            # Store step output in context
            execution.context_data[workflow_step.creates] = result

        # Update any other context variables from result
        if result.get("context_updates"):
            execution.context_data.update(result["context_updates"])

    async def _emit_workflow_event(
        self,
        event_type: EventType,
        project_id: str,
        execution_id: str,
        event_data: Dict[str, Any]
    ) -> None:
        """Emit WebSocket event for workflow updates."""
        try:
            event = WebSocketEvent(
                event_type=event_type,
                project_id=UUID(project_id),
                data={
                    "execution_id": execution_id,
                    **event_data
                }
            )

            # Note: In a real implementation, this would emit to WebSocket clients
            logger.debug("Workflow event emitted",
                        event_type=event_type.value,
                        project_id=project_id,
                        execution_id=execution_id)

        except Exception as e:
            logger.warning("Failed to emit workflow event",
                          event_type=event_type.value,
                          error=str(e))

    async def _check_hitl_triggers_after_step(
        self,
        execution: WorkflowExecutionStateModel,
        step: WorkflowStepExecutionState,
        result: Dict[str, Any]
    ) -> Optional['HitlRequest']:
        """
        Check for HITL triggers after step completion.

        Args:
            execution: Workflow execution state
            step: Completed workflow step
            result: Step execution result

        Returns:
            HitlRequest if trigger condition met, None otherwise
        """
        try:
            # Get project oversight level
            oversight_level = self.hitl_service.get_oversight_level(execution.project_id)

            # Prepare trigger context
            trigger_context = {
                "oversight_level": oversight_level,
                "current_phase": self._get_current_workflow_phase(execution),
                "step_index": step.step_index,
                "agent_type": step.agent,
                "confidence_score": result.get("confidence_score", 0.8),
                "error_type": result.get("error_type"),
                "conflict_detected": result.get("conflict_detected", False),
                "auto_resolution_attempts": result.get("auto_resolution_attempts", 0),
                "budget_usage_percent": result.get("budget_usage_percent", 0),
                "violation_type": result.get("safety_violation_type"),
                "execution_id": execution.execution_id,
                "workflow_id": execution.workflow_id
            }

            # Check for HITL triggers
            hitl_request = await self.hitl_service.check_hitl_triggers(
                project_id=UUID(execution.project_id),
                task_id=UUID(step.task_id) if step.task_id else uuid4(),
                agent_type=step.agent,
                trigger_context=trigger_context
            )

            if hitl_request:
                logger.info("HITL trigger activated after step completion",
                           execution_id=execution.execution_id,
                           step_index=step.step_index,
                           agent=step.agent,
                           hitl_request_id=str(hitl_request.request_id))

            return hitl_request

        except Exception as e:
            logger.error("Failed to check HITL triggers after step",
                        execution_id=execution.execution_id,
                        step_index=step.step_index,
                        error=str(e))
            return None

    def _get_current_workflow_phase(self, execution: WorkflowExecutionStateModel) -> str:
        """
        Determine the current workflow phase based on execution state.

        Args:
            execution: Workflow execution state

        Returns:
            Current phase name
        """
        # Map step indices to phases (this could be made configurable)
        phase_mapping = {
            0: "discovery",
            1: "plan",
            2: "design",
            3: "build",
            4: "validate",
            5: "launch"
        }

        current_step = execution.current_step
        if current_step < len(phase_mapping):
            return phase_mapping.get(current_step, "unknown")

        return "unknown"

    def _convert_uuids_to_strings(self, data: Any) -> Any:
        """
        Recursively convert UUID objects to strings in nested data structures.

        Args:
            data: Data structure that may contain UUID objects

        Returns:
            Data structure with UUID objects converted to strings
        """
        if isinstance(data, UUID):
            return str(data)
        elif isinstance(data, dict):
            return {key: self._convert_uuids_to_strings(value) for key, value in data.items()}
        elif isinstance(data, list):
            return [self._convert_uuids_to_strings(item) for item in data]
        elif isinstance(data, tuple):
            return tuple(self._convert_uuids_to_strings(item) for item in data)
        else:
            return data

    async def execute_sdlc_workflow(
        self,
        project_id: str,
        workflow_id: str = "sdlc-workflow",
        context_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Execute a complete 6-phase SDLC workflow.

        This method orchestrates the full Software Development Life Cycle from
        Discovery through Launch, with proper phase gates and quality controls.

        Args:
            project_id: ID of the project
            workflow_id: ID of the SDLC workflow (defaults to standard SDLC)
            context_data: Initial context data for the workflow

        Returns:
            Dictionary with complete SDLC execution results

        Raises:
            ValueError: If SDLC workflow execution fails
        """
        from app.models.workflow import SDLCWorkflowDefinition, SDLCPhase

        try:
            logger.info("Starting SDLC workflow execution", project_id=project_id, workflow_id=workflow_id)

            # Create SDLC workflow definition
            sdlc_workflow = SDLCWorkflowDefinition(
                id=workflow_id,
                name="Software Development Life Cycle",
                description="Complete 6-phase SDLC workflow with quality gates",
                project_types=["web", "mobile", "api", "fullstack"]
            )

            # Validate SDLC compliance
            compliance_issues = sdlc_workflow.validate_sdlc_compliance()
            if compliance_issues:
                logger.warning("SDLC compliance issues found", issues=compliance_issues)

            # Start workflow execution
            execution = await self.start_workflow_execution(
                workflow_id=workflow_id,
                project_id=project_id,
                context_data=context_data or {}
            )

            # Execute each SDLC phase
            phase_results = {}
            current_step = 0

            while current_step < len(sdlc_workflow.sequence):
                try:
                    # Get current phase
                    current_phase = sdlc_workflow.get_sdlc_phase(current_step)
                    phase_name = current_phase.value if current_phase else f"step_{current_step}"

                    logger.info("Executing SDLC phase",
                               phase=phase_name,
                               step_index=current_step,
                               project_id=project_id)

                    # Execute workflow step
                    step_result = await self.execute_workflow_step(
                        execution_id=execution.execution_id,
                        step_index=current_step
                    )

                    if step_result.get("status") == "paused_for_hitl":
                        # Workflow paused for human approval
                        logger.info("SDLC workflow paused for HITL approval",
                                   phase=phase_name,
                                   hitl_request_id=step_result.get("hitl_request_id"))
                        return {
                            "status": "paused_for_hitl",
                            "execution_id": execution.execution_id,
                            "current_phase": phase_name,
                            "hitl_request_id": step_result.get("hitl_request_id"),
                            "completed_phases": list(phase_results.keys())
                        }

                    # Store phase results
                    phase_results[phase_name] = step_result

                    # Check if we can proceed to next phase
                    if not sdlc_workflow.can_proceed_to_next_phase(current_step, step_result):
                        logger.warning("Cannot proceed to next SDLC phase",
                                      phase=phase_name,
                                      step_index=current_step,
                                      reason="Phase requirements not met")

                        # Pause workflow for manual review
                        await self.pause_workflow_execution(
                            execution_id=execution.execution_id,
                            reason=f"Phase {phase_name} requirements not met - manual review required"
                        )

                        return {
                            "status": "paused_for_review",
                            "execution_id": execution.execution_id,
                            "current_phase": phase_name,
                            "issue": "Phase requirements not met",
                            "completed_phases": list(phase_results.keys())
                        }

                    # Move to next phase
                    current_step += 1

                    # Emit phase completion event
                    await self._emit_sdlc_phase_event(
                        execution.project_id,
                        execution.execution_id,
                        phase_name,
                        "completed",
                        step_result
                    )

                except Exception as e:
                    logger.error("SDLC phase execution failed",
                                phase=phase_name if 'phase_name' in locals() else f"step_{current_step}",
                                step_index=current_step,
                                error=str(e))

                    # Mark execution as failed
                    await self.cancel_workflow_execution(
                        execution_id=execution.execution_id,
                        reason=f"SDLC phase {phase_name} failed: {str(e)}"
                    )

                    return {
                        "status": "failed",
                        "execution_id": execution.execution_id,
                        "failed_phase": phase_name if 'phase_name' in locals() else f"step_{current_step}",
                        "error": str(e),
                        "completed_phases": list(phase_results.keys())
                    }

            # All phases completed successfully
            logger.info("SDLC workflow completed successfully",
                       project_id=project_id,
                       total_phases=len(sdlc_workflow.sequence),
                       execution_id=execution.execution_id)

            # Calculate final metrics
            final_metrics = self._calculate_sdlc_metrics(phase_results)

            return {
                "status": "completed",
                "execution_id": execution.execution_id,
                "total_phases": len(sdlc_workflow.sequence),
                "completed_phases": list(phase_results.keys()),
                "phase_results": phase_results,
                "final_metrics": final_metrics,
                "artifacts_created": execution.created_artifacts
            }

        except Exception as e:
            logger.error("SDLC workflow execution failed",
                        project_id=project_id,
                        workflow_id=workflow_id,
                        error=str(e))
            raise ValueError(f"SDLC workflow execution failed: {str(e)}")

    async def _emit_sdlc_phase_event(
        self,
        project_id: str,
        execution_id: str,
        phase: str,
        event_type: str,
        phase_data: Dict[str, Any]
    ) -> None:
        """Emit WebSocket event for SDLC phase updates."""
        try:
            event_data = {
                "execution_id": execution_id,
                "phase": phase,
                "event": event_type,
                "phase_data": phase_data,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }

            await self._emit_workflow_event(
                EventType.WORKFLOW_EVENT,
                project_id,
                execution_id,
                event_data
            )

        except Exception as e:
            logger.warning("Failed to emit SDLC phase event",
                          phase=phase,
                          event_type=event_type,
                          error=str(e))

    def _calculate_sdlc_metrics(self, phase_results: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate final SDLC workflow metrics."""

        metrics = {
            "total_phases": len(phase_results),
            "successful_phases": 0,
            "failed_phases": 0,
            "average_quality_score": 0.0,
            "total_artifacts": 0,
            "phase_durations": {},
            "quality_scores": {}
        }

        quality_scores = []

        for phase_name, phase_result in phase_results.items():
            if phase_result.get("status") == "completed":
                metrics["successful_phases"] += 1

                # Collect quality scores
                if "confidence_score" in phase_result:
                    quality_scores.append(phase_result["confidence_score"])
                    metrics["quality_scores"][phase_name] = phase_result["confidence_score"]

                # Count artifacts
                artifacts = phase_result.get("result", {}).get("artifacts", [])
                metrics["total_artifacts"] += len(artifacts)

            else:
                metrics["failed_phases"] += 1

        # Calculate averages
        if quality_scores:
            metrics["average_quality_score"] = sum(quality_scores) / len(quality_scores)

        return metrics

    def get_sdlc_workflow_status(self, execution_id: str) -> Optional[Dict[str, Any]]:
        """
        Get detailed status of an SDLC workflow execution.

        Args:
            execution_id: ID of the workflow execution

        Returns:
            Dictionary with SDLC-specific status information
        """
        from app.models.workflow import SDLCWorkflowDefinition, SDLCPhase

        # Get base workflow status
        base_status = self.get_workflow_execution_status(execution_id)
        if not base_status:
            return None

        try:
            # Load SDLC workflow definition
            sdlc_workflow = SDLCWorkflowDefinition(id=base_status["workflow_id"])

            # Add SDLC-specific information
            current_step = base_status.get("current_step", 0)
            current_phase = sdlc_workflow.get_sdlc_phase(current_step)

            phase_requirements = {}
            if current_phase:
                phase_requirements = sdlc_workflow.get_phase_requirements(current_phase)

            progress_percentage = sdlc_workflow.get_sdlc_progress_percentage(current_step)

            sdlc_status = {
                **base_status,
                "current_sdlc_phase": current_phase.value if current_phase else None,
                "phase_requirements": phase_requirements,
                "sdlc_progress_percentage": progress_percentage,
                "next_phase": sdlc_workflow.get_sdlc_phase(current_step + 1).value if current_step + 1 < len(sdlc_workflow.sequence) else None,
                "sdlc_compliance": sdlc_workflow.validate_sdlc_compliance()
            }

            return sdlc_status

        except Exception as e:
            logger.error("Failed to get SDLC workflow status",
                        execution_id=execution_id,
                        error=str(e))
            return base_status
