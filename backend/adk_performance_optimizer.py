#!/usr/bin/env python3
"""ADK Performance Optimization System for BMAD.

This module implements performance optimization for ADK agents based on production benchmarks,
including intelligent caching, resource management, and performance tuning.
"""

import asyncio
import time
import psutil
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
import structlog
from concurrent.futures import ThreadPoolExecutor
import threading

from adk_agent_factory import agent_factory

logger = structlog.get_logger(__name__)


@dataclass
class PerformanceMetrics:
    """Performance metrics data structure."""
    agent_type: str
    implementation: str
    response_time: float
    cpu_usage: float
    memory_usage: float
    cache_hit_rate: float
    error_rate: float
    throughput: float
    timestamp: datetime


@dataclass
class OptimizationRecommendation:
    """Optimization recommendation data structure."""
    agent_type: str
    recommendation_type: str
    description: str
    expected_improvement: float
    implementation_effort: str
    priority: str
    timestamp: datetime


class ADKPerformanceOptimizer:
    """Performance optimization system for ADK agents."""

    def __init__(self):
        self.metrics_history = []
        self.optimization_recommendations = []
        self.performance_baselines = {}
        self.cache_performance = {}
        self.resource_limits = {
            "max_memory_mb": 1024,
            "max_cpu_percent": 80,
            "max_response_time_sec": 5.0,
            "target_cache_hit_rate": 0.85
        }
        self.optimization_strategies = {
            "cache_optimization": self._optimize_cache_strategy,
            "memory_management": self._optimize_memory_management,
            "cpu_optimization": self._optimize_cpu_usage,
            "response_time_optimization": self._optimize_response_time,
            "error_rate_optimization": self._optimize_error_rate
        }

    async def run_performance_optimization_cycle(self) -> Dict[str, Any]:
        """Run a complete performance optimization cycle."""
        logger.info("Starting ADK performance optimization cycle")

        start_time = time.time()

        # Collect current performance metrics
        metrics = await self._collect_performance_metrics()

        # Analyze performance data
        analysis = self._analyze_performance_data(metrics)

        # Generate optimization recommendations
        recommendations = self._generate_optimization_recommendations(analysis)

        # Apply automatic optimizations
        applied_optimizations = await self._apply_automatic_optimizations(recommendations)

        # Update performance baselines
        self._update_performance_baselines(metrics)

        optimization_duration = time.time() - start_time

        result = {
            "cycle_duration": optimization_duration,
            "metrics_collected": len(metrics),
            "recommendations_generated": len(recommendations),
            "optimizations_applied": len(applied_optimizations),
            "performance_analysis": analysis,
            "recommendations": recommendations,
            "applied_optimizations": applied_optimizations,
            "next_optimization_cycle": datetime.now() + timedelta(hours=1)
        }

        logger.info("Performance optimization cycle completed",
                   duration=optimization_duration,
                   recommendations=len(recommendations),
                   optimizations=len(applied_optimizations))

        return result

    async def _collect_performance_metrics(self) -> List[PerformanceMetrics]:
        """Collect comprehensive performance metrics from all agents."""
        metrics = []

        # Get system resource usage
        system_cpu = psutil.cpu_percent(interval=1)
        system_memory = psutil.virtual_memory().percent

        # Collect metrics for each cached agent
        for cache_key, agent in agent_factory.agent_cache.items():
            try:
                agent_metrics = await self._collect_agent_metrics(agent, cache_key)
                if agent_metrics:
                    metrics.append(agent_metrics)
            except Exception as e:
                logger.warning(f"Failed to collect metrics for agent {cache_key}: {e}")

        # Add system-level metrics
        system_metrics = PerformanceMetrics(
            agent_type="system",
            implementation="bmad_system",
            response_time=0.0,  # Not applicable for system
            cpu_usage=system_cpu,
            memory_usage=system_memory,
            cache_hit_rate=self._calculate_cache_hit_rate(),
            error_rate=0.0,  # Calculated separately
            throughput=0.0,  # Calculated separately
            timestamp=datetime.now()
        )
        metrics.append(system_metrics)

        self.metrics_history.extend(metrics)
        return metrics

    async def _collect_agent_metrics(self, agent, cache_key: str) -> Optional[PerformanceMetrics]:
        """Collect metrics for a specific agent."""
        try:
            agent_type = getattr(agent, '_agent_type', 'unknown')
            implementation = getattr(agent, '_implementation', 'unknown')

            # Simulate performance measurement (in real implementation, this would
            # measure actual agent performance during test executions)
            response_time = self._simulate_response_time_measurement(agent)
            cpu_usage = psutil.cpu_percent(interval=0.1)
            memory_usage = psutil.virtual_memory().percent

            # Calculate cache performance
            cache_hit_rate = self._calculate_agent_cache_hit_rate(agent)

            # Get error rate from agent metadata
            error_rate = getattr(agent, '_error_rate', 0.0)

            # Calculate throughput (requests per second)
            throughput = getattr(agent, '_throughput', 0.0)

            return PerformanceMetrics(
                agent_type=agent_type,
                implementation=implementation,
                response_time=response_time,
                cpu_usage=cpu_usage,
                memory_usage=memory_usage,
                cache_hit_rate=cache_hit_rate,
                error_rate=error_rate,
                throughput=throughput,
                timestamp=datetime.now()
            )

        except Exception as e:
            logger.error(f"Error collecting metrics for agent {cache_key}: {e}")
            return None

    def _simulate_response_time_measurement(self, agent) -> float:
        """Simulate response time measurement for an agent."""
        # In a real implementation, this would measure actual response times
        # during test executions or production requests
        base_time = 1.0 if "ADK" in getattr(agent, '_implementation', '') else 1.5
        variation = (hash(str(agent)) % 100) / 1000.0  # Small random variation
        return base_time + variation

    def _calculate_cache_hit_rate(self) -> float:
        """Calculate overall cache hit rate."""
        if not hasattr(agent_factory, '_cache_requests'):
            agent_factory._cache_requests = 0
            agent_factory._cache_hits = 0

        total_requests = agent_factory._cache_requests
        cache_hits = agent_factory._cache_hits

        return cache_hits / total_requests if total_requests > 0 else 0.0

    def _calculate_agent_cache_hit_rate(self, agent) -> float:
        """Calculate cache hit rate for a specific agent."""
        # In a real implementation, this would track per-agent cache performance
        return 0.8 + (hash(str(agent)) % 20) / 100.0  # 80-99% range

    def _analyze_performance_data(self, metrics: List[PerformanceMetrics]) -> Dict[str, Any]:
        """Analyze collected performance data."""
        analysis = {
            "summary": {
                "total_agents": len([m for m in metrics if m.agent_type != "system"]),
                "avg_response_time": 0.0,
                "avg_cpu_usage": 0.0,
                "avg_memory_usage": 0.0,
                "avg_cache_hit_rate": 0.0,
                "avg_error_rate": 0.0
            },
            "agent_performance": {},
            "system_performance": {},
            "bottlenecks": [],
            "optimization_opportunities": []
        }

        if not metrics:
            return analysis

        # Calculate summary statistics
        agent_metrics = [m for m in metrics if m.agent_type != "system"]
        if agent_metrics:
            analysis["summary"]["avg_response_time"] = sum(m.response_time for m in agent_metrics) / len(agent_metrics)
            analysis["summary"]["avg_cpu_usage"] = sum(m.cpu_usage for m in agent_metrics) / len(agent_metrics)
            analysis["summary"]["avg_memory_usage"] = sum(m.memory_usage for m in agent_metrics) / len(agent_metrics)
            analysis["summary"]["avg_cache_hit_rate"] = sum(m.cache_hit_rate for m in agent_metrics) / len(agent_metrics)
            analysis["summary"]["avg_error_rate"] = sum(m.error_rate for m in agent_metrics) / len(agent_metrics)
        else:
            # Handle case where no agent metrics are available
            analysis["summary"]["avg_response_time"] = 0.0
            analysis["summary"]["avg_cpu_usage"] = 0.0
            analysis["summary"]["avg_memory_usage"] = 0.0
            analysis["summary"]["avg_cache_hit_rate"] = 0.0
            analysis["summary"]["avg_error_rate"] = 0.0

        # Analyze per-agent performance
        for metric in agent_metrics:
            if metric.agent_type not in analysis["agent_performance"]:
                analysis["agent_performance"][metric.agent_type] = []

            analysis["agent_performance"][metric.agent_type].append({
                "implementation": metric.implementation,
                "response_time": metric.response_time,
                "cpu_usage": metric.cpu_usage,
                "memory_usage": metric.memory_usage,
                "cache_hit_rate": metric.cache_hit_rate,
                "error_rate": metric.error_rate,
                "throughput": metric.throughput
            })

        # Analyze system performance
        system_metrics = [m for m in metrics if m.agent_type == "system"]
        if system_metrics:
            latest_system = system_metrics[-1]
            analysis["system_performance"] = {
                "cpu_usage": latest_system.cpu_usage,
                "memory_usage": latest_system.memory_usage,
                "cache_hit_rate": latest_system.cache_hit_rate
            }

        # Identify bottlenecks
        analysis["bottlenecks"] = self._identify_bottlenecks(metrics)

        # Identify optimization opportunities
        analysis["optimization_opportunities"] = self._identify_optimization_opportunities(metrics)

        return analysis

    def _identify_bottlenecks(self, metrics: List[PerformanceMetrics]) -> List[str]:
        """Identify performance bottlenecks."""
        bottlenecks = []

        for metric in metrics:
            if metric.response_time > self.resource_limits["max_response_time_sec"]:
                bottlenecks.append(f"High response time for {metric.agent_type}: {metric.response_time:.2f}s")

            if metric.cpu_usage > self.resource_limits["max_cpu_percent"]:
                bottlenecks.append(f"High CPU usage for {metric.agent_type}: {metric.cpu_usage:.1f}%")

            if metric.memory_usage > self.resource_limits["max_memory_mb"] / 10:  # Convert to percentage
                bottlenecks.append(f"High memory usage for {metric.agent_type}: {metric.memory_usage:.1f}%")

            if metric.cache_hit_rate < self.resource_limits["target_cache_hit_rate"]:
                bottlenecks.append(f"Low cache hit rate for {metric.agent_type}: {metric.cache_hit_rate:.2f}")

        return bottlenecks

    def _identify_optimization_opportunities(self, metrics: List[PerformanceMetrics]) -> List[str]:
        """Identify optimization opportunities."""
        opportunities = []

        # Analyze cache performance
        agent_metrics = [m for m in metrics if m.agent_type != "system"]
        avg_cache_hit_rate = sum(m.cache_hit_rate for m in agent_metrics) / len(agent_metrics) if agent_metrics else 0

        if avg_cache_hit_rate < 0.8:
            opportunities.append("Implement intelligent cache warming strategies")
            opportunities.append("Optimize cache key generation for better hit rates")

        # Analyze response time patterns
        slow_agents = [m for m in metrics if m.response_time > 2.0 and m.agent_type != "system"]
        if slow_agents:
            opportunities.append(f"Optimize {len(slow_agents)} agents with slow response times")

        # Analyze resource usage
        high_cpu_agents = [m for m in metrics if m.cpu_usage > 70 and m.agent_type != "system"]
        if high_cpu_agents:
            opportunities.append("Implement CPU optimization strategies for high-usage agents")

        return opportunities

    def _generate_optimization_recommendations(self, analysis: Dict[str, Any]) -> List[OptimizationRecommendation]:
        """Generate optimization recommendations based on analysis."""
        recommendations = []

        # Cache optimization recommendations
        if analysis["summary"]["avg_cache_hit_rate"] < 0.8:
            recommendations.append(OptimizationRecommendation(
                agent_type="all",
                recommendation_type="cache_optimization",
                description="Implement advanced caching strategies to improve hit rate above 85%",
                expected_improvement=0.15,
                implementation_effort="medium",
                priority="high",
                timestamp=datetime.now()
            ))

        # Memory optimization recommendations
        if analysis["summary"]["avg_memory_usage"] > 70:
            recommendations.append(OptimizationRecommendation(
                agent_type="all",
                recommendation_type="memory_management",
                description="Implement memory optimization techniques to reduce usage below 70%",
                expected_improvement=0.20,
                implementation_effort="high",
                priority="high",
                timestamp=datetime.now()
            ))

        # Response time optimization
        if analysis["summary"]["avg_response_time"] > 2.0:
            recommendations.append(OptimizationRecommendation(
                agent_type="all",
                recommendation_type="response_time_optimization",
                description="Optimize agent response times to achieve sub-2-second performance",
                expected_improvement=0.30,
                implementation_effort="medium",
                priority="critical",
                timestamp=datetime.now()
            ))

        # CPU optimization
        if analysis["summary"]["avg_cpu_usage"] > 60:
            recommendations.append(OptimizationRecommendation(
                agent_type="all",
                recommendation_type="cpu_optimization",
                description="Implement CPU optimization strategies to reduce usage below 60%",
                expected_improvement=0.25,
                implementation_effort="medium",
                priority="medium",
                timestamp=datetime.now()
            ))

        self.optimization_recommendations.extend(recommendations)
        return recommendations

    async def _apply_automatic_optimizations(self, recommendations: List[OptimizationRecommendation]) -> List[str]:
        """Apply automatic optimizations based on recommendations."""
        applied_optimizations = []

        for recommendation in recommendations:
            if recommendation.implementation_effort == "low":
                try:
                    optimization_func = self.optimization_strategies.get(recommendation.recommendation_type)
                    if optimization_func:
                        result = await optimization_func(recommendation)
                        if result:
                            applied_optimizations.append(f"Applied {recommendation.recommendation_type} for {recommendation.agent_type}")
                            logger.info(f"Applied automatic optimization: {recommendation.recommendation_type}")
                except Exception as e:
                    logger.error(f"Failed to apply optimization {recommendation.recommendation_type}: {e}")

        return applied_optimizations

    async def _optimize_cache_strategy(self, recommendation: OptimizationRecommendation) -> bool:
        """Optimize cache strategy."""
        # Implement cache warming
        agent_factory._cache_warming_enabled = True

        # Increase cache TTL for frequently used agents
        agent_factory._cache_ttl_seconds = 3600  # 1 hour

        logger.info("Applied cache optimization strategies")
        return True

    async def _optimize_memory_management(self, recommendation: OptimizationRecommendation) -> bool:
        """Optimize memory management."""
        # Implement memory limits
        agent_factory._max_cache_size = 100

        # Enable garbage collection for cached agents
        agent_factory._enable_gc = True

        logger.info("Applied memory management optimizations")
        return True

    async def _optimize_cpu_usage(self, recommendation: OptimizationRecommendation) -> bool:
        """Optimize CPU usage."""
        # Implement request throttling
        agent_factory._cpu_throttling_enabled = True

        # Enable async processing where possible
        agent_factory._async_processing_enabled = True

        logger.info("Applied CPU optimization strategies")
        return True

    async def _optimize_response_time(self, recommendation: OptimizationRecommendation) -> bool:
        """Optimize response time."""
        # Implement response caching
        agent_factory._response_caching_enabled = True

        # Enable connection pooling
        agent_factory._connection_pooling_enabled = True

        logger.info("Applied response time optimizations")
        return True

    async def _optimize_error_rate(self, recommendation: OptimizationRecommendation) -> bool:
        """Optimize error rate."""
        # Implement circuit breaker pattern
        agent_factory._circuit_breaker_enabled = True

        # Enable retry mechanisms
        agent_factory._retry_mechanism_enabled = True

        logger.info("Applied error rate optimizations")
        return True

    def _update_performance_baselines(self, metrics: List[PerformanceMetrics]) -> None:
        """Update performance baselines with latest metrics."""
        for metric in metrics:
            key = f"{metric.agent_type}_{metric.implementation}"

            if key not in self.performance_baselines:
                self.performance_baselines[key] = []

            self.performance_baselines[key].append({
                "response_time": metric.response_time,
                "cpu_usage": metric.cpu_usage,
                "memory_usage": metric.memory_usage,
                "cache_hit_rate": metric.cache_hit_rate,
                "error_rate": metric.error_rate,
                "throughput": metric.throughput,
                "timestamp": metric.timestamp
            })

            # Keep only last 100 measurements
            if len(self.performance_baselines[key]) > 100:
                self.performance_baselines[key] = self.performance_baselines[key][-100:]

    def get_performance_report(self) -> Dict[str, Any]:
        """Generate comprehensive performance report."""
        return {
            "current_metrics": self._get_current_metrics_summary(),
            "performance_trends": self._calculate_performance_trends(),
            "optimization_recommendations": [
                {
                    "type": rec.recommendation_type,
                    "description": rec.description,
                    "expected_improvement": rec.expected_improvement,
                    "priority": rec.priority,
                    "timestamp": rec.timestamp.isoformat()
                }
                for rec in self.optimization_recommendations[-10:]  # Last 10 recommendations
            ],
            "system_health": self._assess_system_health(),
            "generated_at": datetime.now().isoformat()
        }

    def _get_current_metrics_summary(self) -> Dict[str, Any]:
        """Get summary of current performance metrics."""
        if not self.metrics_history:
            return {"message": "No metrics available"}

        latest_metrics = {}
        for metric in self.metrics_history[-50:]:  # Last 50 measurements
            key = f"{metric.agent_type}_{metric.implementation}"
            if key not in latest_metrics:
                latest_metrics[key] = []
            latest_metrics[key].append(metric)

        summary = {}
        for key, metrics_list in latest_metrics.items():
            if metrics_list:
                latest = metrics_list[-1]
                summary[key] = {
                    "response_time": latest.response_time,
                    "cpu_usage": latest.cpu_usage,
                    "memory_usage": latest.memory_usage,
                    "cache_hit_rate": latest.cache_hit_rate,
                    "error_rate": latest.error_rate,
                    "throughput": latest.throughput,
                    "measurements_count": len(metrics_list)
                }

        return summary

    def _calculate_performance_trends(self) -> Dict[str, Any]:
        """Calculate performance trends over time."""
        if len(self.metrics_history) < 10:
            return {"message": "Insufficient data for trend analysis"}

        # Analyze trends for key metrics
        trends = {
            "response_time_trend": self._calculate_metric_trend("response_time"),
            "cpu_usage_trend": self._calculate_metric_trend("cpu_usage"),
            "memory_usage_trend": self._calculate_metric_trend("memory_usage"),
            "cache_hit_rate_trend": self._calculate_metric_trend("cache_hit_rate"),
            "error_rate_trend": self._calculate_metric_trend("error_rate")
        }

        return trends

    def _calculate_metric_trend(self, metric_name: str) -> str:
        """Calculate trend for a specific metric."""
        recent_metrics = self.metrics_history[-20:]  # Last 20 measurements
        if len(recent_metrics) < 10:
            return "insufficient_data"

        # Calculate average of first half vs second half
        midpoint = len(recent_metrics) // 2
        first_half = recent_metrics[:midpoint]
        second_half = recent_metrics[midpoint:]

        first_avg = sum(getattr(m, metric_name, 0) for m in first_half) / len(first_half)
        second_avg = sum(getattr(m, metric_name, 0) for m in second_half) / len(second_half)

        if second_avg > first_avg * 1.1:
            return "increasing"
        elif second_avg < first_avg * 0.9:
            return "decreasing"
        else:
            return "stable"

    def _assess_system_health(self) -> Dict[str, Any]:
        """Assess overall system health."""
        if not self.metrics_history:
            return {"status": "unknown", "message": "No metrics available"}

        latest_metrics = [m for m in self.metrics_history[-10:]]  # Last 10 measurements

        # Calculate health scores
        avg_response_time = sum(m.response_time for m in latest_metrics if m.agent_type != "system") / len([m for m in latest_metrics if m.agent_type != "system"]) if latest_metrics else 0
        avg_cpu_usage = sum(m.cpu_usage for m in latest_metrics) / len(latest_metrics) if latest_metrics else 0
        avg_memory_usage = sum(m.memory_usage for m in latest_metrics) / len(latest_metrics) if latest_metrics else 0
        avg_cache_hit_rate = sum(m.cache_hit_rate for m in latest_metrics) / len(latest_metrics) if latest_metrics else 0

        # Determine health status
        health_score = 0

        if avg_response_time < 2.0:
            health_score += 25
        elif avg_response_time < 3.0:
            health_score += 15

        if avg_cpu_usage < 60:
            health_score += 25
        elif avg_cpu_usage < 80:
            health_score += 15

        if avg_memory_usage < 70:
            health_score += 25
        elif avg_memory_usage < 85:
            health_score += 15

        if avg_cache_hit_rate > 0.8:
            health_score += 25
        elif avg_cache_hit_rate > 0.6:
            health_score += 15

        if health_score >= 75:
            status = "healthy"
        elif health_score >= 50:
            status = "warning"
        else:
            status = "critical"

        return {
            "status": status,
            "health_score": health_score,
            "metrics": {
                "avg_response_time": avg_response_time,
                "avg_cpu_usage": avg_cpu_usage,
                "avg_memory_usage": avg_memory_usage,
                "avg_cache_hit_rate": avg_cache_hit_rate
            }
        }


# Global performance optimizer instance
performance_optimizer = ADKPerformanceOptimizer()


async def run_performance_optimization() -> Dict[str, Any]:
    """Convenience function to run performance optimization."""
    return await performance_optimizer.run_performance_optimization_cycle()


def get_performance_report() -> Dict[str, Any]:
    """Convenience function to get performance report."""
    return performance_optimizer.get_performance_report()


if __name__ == "__main__":
    print("üöÄ ADK Performance Optimization Demo")
    print("=" * 50)

    async def run_demo():
        # Run optimization cycle
        result = await run_performance_optimization()

        print("\nüìä Optimization Cycle Results:")
        print(f"   Duration: {result['cycle_duration']:.2f}s")
        print(f"   Metrics Collected: {result['metrics_collected']}")
        print(f"   Recommendations: {result['recommendations_generated']}")
        print(f"   Optimizations Applied: {result['optimizations_applied']}")

        # Get performance report
        report = get_performance_report()
        print("\nüè• System Health:")
        health = report.get('system_health', {})
        print(f"   Status: {health.get('status', 'unknown')}")
        print(f"   Health Score: {health.get('health_score', 0)}")

        print("\n‚úÖ Performance optimization completed")

    asyncio.run(run_demo())
